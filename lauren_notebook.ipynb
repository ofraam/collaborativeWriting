{
 "metadata": {
  "name": "",
  "signature": "sha256:e3ccaf1307ff9161a4983bcb13e5dc36390910a828f158715ed2eb6a1b1699de"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline \n",
      "#imports\n",
      "import cPickle, copy, custom_texttiling as ct, difflib, gensim, jellyfish, matplotlib.pyplot as plt, matplotlib.cm as cm\n",
      "import Levenshtein, nltk, nltk.data, numpy as np, os, re\n",
      "\n",
      "\n",
      "from pylab import gca, Rectangle\n",
      "from difflib import SequenceMatcher\n",
      "from gensim import corpora, models, similarities\n",
      "from nltk.corpus import stopwords\n",
      "from matplotlib import rcParams\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#colorbrewer2 Dark2 qualitative color table\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'white'\n",
      "rcParams['patch.facecolor'] = dark2_colors[0]\n",
      "rcParams['font.family'] = 'StixGeneral'\n",
      "\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The stopword pickle contains a list of english stopword we want to filter out when tokenizing our text. The english pickle contains information about sentence endings and will get used for splitting text into sentences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#used for splitting function\n",
      "stop = stopwords.words('english')\n",
      "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Preprocessing\n",
      "\n",
      "###Steps done before looking at the metrics\n",
      "\n",
      "1. download xml from wikipedia\n",
      "2. parse it and clean the text of each revision using regular expressions to delete wikipedia specific markup\n",
      "3. store the extra information (date, author, comment) for each revision \n",
      "4. create a page object to store the array of revisions\n",
      "5. for each set of text, author, date, comment add a revision object\n",
      "6. start reducing the size of the set - delete revisions with length < 150 chars\n",
      "7. delete all revisions with not enough changes (if neighbors have Levenshtein distance <15 delete the newer one). Only runs once\n",
      "8. create paragraph objects in each version object. split it into paragraphs and only concatenate again if there is no stopsign at the end of a paragraph. \n",
      "\n",
      "\n",
      "###Classes\n",
      "\n",
      "<b>class Paragraph</b>: \n",
      "\n",
      "- variables: text, nextindex, lastindex, changed\n",
      "- functions: None\n",
      "\n",
      "<b>class Version</b>: \n",
      "\n",
      "- variables: text, (comment), date, (author), paragraphs\n",
      "- functions: showDate()\n",
      "\n",
      "\n",
      "<b>class Page</b>: \n",
      "\n",
      "- variables: title, revisions\n",
      "- functions: get_all_text(), get_all_dates(), get_all_authors(), get_all_paragraphs()\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#class is used to locally store the lists of all similarities between paragraphs to save runtime when rerunning the script\n",
      "class store_max_lists():\n",
      "    def __init__(self, lev, sm, cosine, jelly):\n",
      "        self.lev = lev\n",
      "        self.sm = sm\n",
      "        self.cosine = cosine\n",
      "        self.jelly = jelly"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#class for prediction evaluation\n",
      "class prediction_pickle():\n",
      "    def __init__(self):\n",
      "        self.versions= None\n",
      "        self.same_after = None\n",
      "        self.all_after = None\n",
      "        self.all_before_after = None\n",
      "        self.all_avg_before_after = None\n",
      "        self.neighbor_after = None\n",
      "        self.neighbor_before_after_values_indices = None\n",
      "        self.neighbor_avg_before_after = None\n",
      "        self.xscatter_initial_n = None\n",
      "        self.yscatter_neighbor = None\n",
      "        self.xscatter_initial_a = None\n",
      "        self.yscatter_all = None\n",
      "        self.future_linked = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functions\n",
      "=====\n",
      "\n",
      "1. get_pickle: open a pickle file from the pickles subfolder\n",
      "        get_pickle('savant_syndrom.pkl')\n",
      "2. store_text: if manipulated, it will store a text as .txt (use it to manually look at text).\n",
      "        store_text(845, 'savant845.txt')\n",
      "3. generate_ratios: Will compute and return a list of lev ratios between two arrays of texts (paragraphs). Normally maps old to new paragraphs. If option reverse is True, it will map new to old. Parameter sim_func gets a similarity function that will get used. Default is levenshtein.\n",
      "        lev_dists = generate_ratios(curr_paras[1], curr_paras[100])\n",
      "4. generate_all_ratios: Will call generate_ratios for all paragraphs in the list and return a list of max ratios between them. Parameter how is the similarity function used in generate_ratios\n",
      "\n",
      "## Get the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load data\n",
      "def get_pickle(pickle_file, folder=\"pickles\"):\n",
      "    pickle_name = os.path.join(os.getcwd(), folder, pickle_file)\n",
      "    pkl_file = open(pickle_name, 'rb')\n",
      "    xml_parse = cPickle.load(pkl_file)\n",
      "    pkl_file.close()\n",
      "    return xml_parse\n",
      "    \n",
      "#save data\n",
      "def store_text(index, filename):\n",
      "    text_file = open(os.path.join(os.getcwd(), \"texts\", filename),'w')\n",
      "    text_file.write(merkel_text[index].encode(\"UTF-8\"))\n",
      "    text_file.close()\n",
      "    \n",
      "'''\n",
      "bunch of similarity functions that return the similarity ratio\n",
      "'''\n",
      "    \n",
      "def sequence_sim(a, b):\n",
      "    return SequenceMatcher(None, a, b).ratio()\n",
      "\n",
      "def nltk_sim(a, b):\n",
      "    return nltk.metrics.edit_distance(a, b)\n",
      "\n",
      "def jelly_sim(a, b):\n",
      "    return 1-jellyfish.levenshtein_distance(a, b)/float(max(len(a), len(b)))\n",
      "\n",
      "def cosine_sim(a, b):\n",
      "    tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
      "    tfidf_matrix_train1 = tfidf_vectorizer.fit_transform((a, b))\n",
      "    return cosine_similarity(tfidf_matrix_train1[0:1], tfidf_matrix_train1)[0][1]\n",
      "\n",
      "def lev_sim(a, b):\n",
      "    return Levenshtein.ratio(a, b)\n",
      "\n",
      "def generate_ratios(paras1, paras2, reverse=False, sim_func=lev_sim):\n",
      "    '''\n",
      "    generates a list of lev distances between two lists of paragraphs\n",
      "    return: list of list of lev ratios\n",
      "    '''         \n",
      "    \n",
      "    dists = []\n",
      "    if reverse:\n",
      "        for a in paras2:\n",
      "            lev_for_a = []\n",
      "            for b in paras1:\n",
      "                lev_for_a.append(sim_func(a, b))\n",
      "            dists.append(lev_for_a)\n",
      "    else:\n",
      "        for a in paras1:\n",
      "            lev_for_a = []\n",
      "            for b in paras2:\n",
      "                lev_for_a.append(sim_func(a, b))\n",
      "            dists.append(lev_for_a)\n",
      "    return dists\n",
      "\n",
      "def generate_all_ratios(how=lev_sim):\n",
      "    max_ratio_list = []\n",
      "    for i in xrange(len(current_paras)):\n",
      "        if i < len(current_paras)-1:\n",
      "            dists_list = generate_ratios(current_paratexts[i], current_paratexts[i+1], reverse=False, sim_func=how)\n",
      "            for a in dists_list:\n",
      "                max_ratio_list.append( max(a))\n",
      "    return max_ratio_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tilemode = False\n",
      "store_pickle = False\n",
      "\n",
      "if tilemode:\n",
      "    tile_offset = 4\n",
      "else:\n",
      "    tile_offset = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sp0\n",
      "#load necessary data\n",
      "pickle_file_name = 'machine_learning.pkl'\n",
      "current_pickle = get_pickle(pickle_file_name)\n",
      "current_texts = current_pickle.get_all_text()\n",
      "current_paras = current_pickle.get_all_paragraphs()\n",
      "current_paratexts = [[a.text.encode('utf-8') for a in b] for b in current_paras]\n",
      "tt = ct.TextTilingTokenizer()\n",
      "#may wanna pickle that data...\n",
      "if tilemode:\n",
      "    current_tiles = [tt.tokenize(v) for v in ['\\n'.join(a) for a in current_paratexts[tile_offset:]]]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#check if code works with tiles instead of paras\n",
      "if tilemode:\n",
      "    current_paratexts2 = current_paratexts\n",
      "    current_paratexts = current_tiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#splits the texts of the paragraphs into sentences. detects punctuation properly using the english pickle\n",
      "current_sentences = [[sent_detector.tokenize(a.strip()) for a in para] for para in current_paratexts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Mapping of paragaphs over time"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mappings are generated version by version. The first step is computing the levenshtein distance between each pair of paragraph. That means assuming that we have two versions with two paragraphs each we will have to compute four values: (1 - 1), (1 - 2), (2 - 1), (2 - 2). The maximum value for each paragraph of the first version together with the index of the max value will get stored.\n",
      "Afterwards we check whether this max value is below a certain threshold and in this case throw it away (indication this is a deleted paragraph). \n",
      "\n",
      "After we have generated each mapping it could be the case that two paragaphs map to the same paragraph in the next version. Thats why we have to generate the mapping in both directions and then only assign a paragaph as mapped in the case that the respective mapping of the assigned paragaph backwards is to the same paragaph. This means that if we have the mapping (1 - 2) and (2 - 2), the backwards assignment (2 - 1), the final mapping will be between 1 and 2.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#generates mapping between two paragraphs based on the lev dist between every combination of paragraph mappings\n",
      "def assign_neighbors(version1, version2, delthreshhold):\n",
      "    lev_dists = generate_ratios(version1, version2, reverse=False)\n",
      "    mappings = {}\n",
      "    for index1 in xrange(len(lev_dists)):\n",
      "        index2, val = max([(i,x) for (i,x) in enumerate(lev_dists[index1])],key=lambda a:a[1])\n",
      "        # if old paragraph maps to no new paragraphs, make note that it shouldnt be used\n",
      "        if val < delthreshhold:\n",
      "            mappings[index1] = None\n",
      "        # if old paragraph maps well to a new paragraph, make note not to use anymore\n",
      "        else:\n",
      "            mappings[index1] = index2\n",
      "    return mappings\n",
      "\n",
      "#previously 60 and 90\n",
      "check1 = 0\n",
      "check2 = 0\n",
      "\n",
      "\n",
      "mf = assign_neighbors(current_paratexts[check1], current_paratexts[check2], .4)\n",
      "mb = assign_neighbors(current_paratexts[check2], current_paratexts[check1], .4)\n",
      "\n",
      "print mf\n",
      "print mb\n",
      "\n",
      "def generate_mapping(mf,mb):\n",
      "    new_mappings = {}\n",
      "    for (key,value) in mf.iteritems():\n",
      "        if value != None:\n",
      "            if mb[value] == key:\n",
      "                new_mappings[key] = value\n",
      "    return new_mappings\n",
      "        \n",
      "nm = generate_mapping(mf, mb)\n",
      "print nm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{0: 0, 1: 1}\n",
        "{0: 0, 1: 1}\n",
        "{0: 0, 1: 1}\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the above described method we create the mapping between each neighbor-version. Afterwards we filter out mappings that are perfect matches (Levenshtein ratio of 1 means that we have no change). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#generate a list of all mappings, threshold was empirically chosen as .4\n",
      "all_mappings = []\n",
      "for i in xrange(len(current_paratexts)):\n",
      "    if i < len(current_paratexts)-1:\n",
      "        m_f = assign_neighbors(current_paratexts[i], current_paratexts[i+1], .4)\n",
      "        m_b = assign_neighbors(current_paratexts[i+1], current_paratexts[i], .4)\n",
      "        all_mappings.append(generate_mapping(m_f, m_b))\n",
      "print all_mappings[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# mapping threshhold function sp1\n",
      "#take the subset of all mappings where the levenshtein ratio is below 1 (imperfect mappings)\n",
      "def detect_mapping_changes(t):\n",
      "    all_mappings_with_changes = []\n",
      "    all_mappings_without_changes = []\n",
      "    for i, v in enumerate(all_mappings):\n",
      "        changed_mappings = {}\n",
      "        not_changed_mappings = {}\n",
      "        for key, value in v.iteritems():\n",
      "            if Levenshtein.ratio(current_paratexts[i][key], current_paratexts[i+1][value]) < t:\n",
      "                changed_mappings[key] = value\n",
      "            else:\n",
      "                not_changed_mappings[key] = value\n",
      "        all_mappings_with_changes.append(changed_mappings)\n",
      "        all_mappings_without_changes.append(not_changed_mappings)\n",
      "    return all_mappings_with_changes, all_mappings_without_changes\n",
      "\n",
      "all_mappings_with_changes,all_mappings_without_changes = detect_mapping_changes(1.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Topic modelling\n",
      "\n",
      "###create model and corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to create a topic model we need to flatten our twodimensional array of version and paragraph to a list of texts. We create the helper array version_para_to_index in oder to keep track of the new indices in the list. We additionally split each text in the list into lower case words and throw away every stopword and parsing errors that still have \"http:\" in them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1 - tokenize the text. version_para_to_index will get used to keep track of indices because we add all paragraphs to a long list\n",
      "list_of_paratexts = []\n",
      "iterator = 0\n",
      "version_para_to_index = {}\n",
      "for i in xrange(len(current_paratexts)):\n",
      "    pararange = xrange(len(current_paratexts[i]))\n",
      "    for j in pararange:\n",
      "        list_of_paratexts.append(current_paratexts[i][j])\n",
      "        version_para_to_index[(i,j)] = iterator\n",
      "        iterator += 1\n",
      "        \n",
      "#1.1 add all words to a list and throw stopwords away. stop is a list of stopwords (defined at top)\n",
      "list_of_filtered_paratexts = [[word for word in document.lower().split() if word not in stop]\n",
      "         for document in  list_of_paratexts]\n",
      "#1.2 filter out http from paragraphs\n",
      "list_of_filtered_paratexts = [[word for word in doc if 'http:' not in word] for doc in list_of_filtered_paratexts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1.3 - tokenize the text. use only words that occur more than once\n",
      "all_tokens = sum(list_of_filtered_paratexts, [])\n",
      "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
      "texts = [[word for word in text if word not in tokens_once]\n",
      "         for text in list_of_filtered_paratexts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#2. create a dictionary from the tokens (assign an ID to each word)\n",
      "dictionary = corpora.Dictionary(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#3. transform dict to vector and then to Tf-idf\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "tfidf = models.TfidfModel(corpus)\n",
      "corpus_tfidf = tfidf[corpus]\n",
      "\n",
      "#4. transform the Tf-idf into a LSI-topic model\n",
      "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary) # initialize an LSI transformation\n",
      "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
      "index = similarities.MatrixSimilarity(lsi[corpus])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Compute topic similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the above created Lsi model we can now compare paragaphs. We do this for each pair of assigned paragraphs and store the topic similarity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_mappings_changed_scores = []\n",
      "for i, version in enumerate(all_mappings_with_changes):\n",
      "    curr_dict = {}\n",
      "    for key, value in version.iteritems():\n",
      "        new = version_para_to_index[(i+1,value)]\n",
      "        vec_bow=dictionary.doc2bow(current_paratexts[i][key].lower().split())\n",
      "        vec_lsi= lsi[vec_bow]\n",
      "        score = index[vec_lsi]\n",
      "        curr_dict[key] = score[new]\n",
      "    all_mappings_changed_scores.append(curr_dict)\n",
      "print all_mappings_changed_scores[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to have a closer look at the texts with significant changes. We empirically determined (we looked at the histogram) a significant change to be a ratio of <.8."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "significant_changes = []\n",
      "for i, version in enumerate(all_mappings_changed_scores):\n",
      "    for key, value in version.iteritems():\n",
      "        if value < .8:\n",
      "            old_para = current_paratexts[i][key]\n",
      "            new_index = all_mappings_with_changes[i][key]\n",
      "            new_para = current_paratexts[i+1][new_index]\n",
      "            significant_changes.append((old_para, new_para, value))\n",
      "significant_changes.sort(key=lambda x: x[2])\n",
      "print \"There are\", len(significant_changes), \"significant changes\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#helper function to make the array readable by humans\n",
      "def show_changed_para(index):\n",
      "    curr_change = significant_changes[index]\n",
      "    print \"The following mapped paragraphs had a topic similarity of\", curr_change[2]\n",
      "    print \"---------------------------------------------\"\n",
      "    print \"OLD:\"\n",
      "    print curr_change[0]\n",
      "    print \"---------------------------------------------\"\n",
      "    print \"NEW:\"\n",
      "    print curr_change[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_changed_para(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bad examples I looked at: \n",
      "\n",
      "- 0: not related, don't know how this got matched\n",
      "- 1, 2: related, in one of them there is a parsing error\n",
      "- 5, 6: looks like they split the paragraph\n",
      "- 7: a paragraph in process was continued\n",
      "- 60, 65, 80: vandalism\n",
      "- 75: just deletion of 1 sentence\n",
      "- 85: a discussion within an edit?!\n",
      "\n",
      "Good examples:\n",
      "\n",
      "- 3: completely new topic!\n",
      "- 4: opposite meaning?!\n",
      "- 8: completely different\n",
      "- 9: change of topic\n",
      "- 10: just two words difference but it means something different\n",
      "- 11, 90: just different\n",
      "- 70: really interesting subtle change\n",
      "\n",
      "\n",
      "other: I noticed that the \"according to treffert\" paragraph is in the list with heavy changes a lot of times. Do we want to define something that can pick this up? It's clearly a controversial paragraph"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Coherence Calculations\n",
      "\n",
      "###whole text to para"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first attempt is looking at the coherence. We can do this using topic similarity again. Our hypothesis is that a coherent paragraph will have the same keywords as the whole text. This is why we tokenize every whole text and compare it to each paragraph that gets changed.\n",
      "\n",
      "For the evaluation we tried filtering out short paragraphs (<4 sentences). This did not improve the quality of the result"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tokens for each version\n",
      "list_of_filtered_versions = [[word for word in document.lower().split() if word not in stop]\n",
      "         for document in  current_texts]\n",
      "all_tokens_versions = sum(list_of_filtered_versions, [])\n",
      "tokens_once_versions = set(word for word in set(all_tokens_versions) if all_tokens_versions.count(word) == 1)\n",
      "texts_versions = [[word for word in text if word not in tokens_once_versions]\n",
      "         for text in list_of_filtered_versions]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#compare paragraphs of a version with respective version\n",
      "all_para_version_scores = []\n",
      "all_para_version_scores_filtered = []\n",
      "all_para_version_scores_without_changes = []\n",
      "for i, version in enumerate(all_mappings_with_changes):\n",
      "    curr_dict = {}\n",
      "    filtered_dict={}\n",
      "    all_dict = {}\n",
      "    indices_of_changed_paragraphs = version.keys() \n",
      "    #we need all coherence scores for a version for later. Above is most efficient just for this test\n",
      "    indices_of_paragraphs = xrange(len(current_paratexts[i]))\n",
      "    vec_bow=dictionary.doc2bow(texts_versions[i])\n",
      "    vec_lsi= lsi[vec_bow]\n",
      "    score = index[vec_lsi]\n",
      "    for key in indices_of_paragraphs:\n",
      "        all_dict[key] = score[version_para_to_index[(i,key)]]\n",
      "        if key in indices_of_changed_paragraphs:\n",
      "            #try looking only at long paragraphs\n",
      "            if len(current_sentences[i][key]) > 4:\n",
      "                filtered_dict[key] = score[version_para_to_index[(i,key)]]\n",
      "            curr_dict[key] = score[version_para_to_index[(i,key)]]\n",
      "    all_para_version_scores_filtered.append(filtered_dict)\n",
      "    all_para_version_scores.append(curr_dict)\n",
      "    all_para_version_scores_without_changes.append(all_dict)\n",
      "print all_para_version_scores[:20]\n",
      "\n",
      "#compare paragraphs with before and after paragraphs (optional)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "non_coherent_paras = []\n",
      "for i, version in enumerate(all_para_version_scores_filtered):\n",
      "    for key, value in version.iteritems():\n",
      "        if value < .4:\n",
      "            non_coherent_paras.append((i, key, value))\n",
      "non_coherent_paras.sort(key=lambda x: x[2])\n",
      "print \"There are\", len(non_coherent_paras), \"non coherent paras\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#helper function to make the array readable by humans\n",
      "def show_incoherent_para(index):\n",
      "    curr_change = non_coherent_paras[index]\n",
      "    print \"The following mapped paragraphs had a topic similarity of\", curr_change[2]\n",
      "    print \"---------------------------------------------\"\n",
      "    print \"Whole text:\"\n",
      "    print current_texts[curr_change[0]+tile_offset]\n",
      "    print \"---------------------------------------------\"\n",
      "    print \"Specific paragraph:\"\n",
      "    print current_paratexts[curr_change[0]][curr_change[1]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_incoherent_para(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###change of whole text to para between versions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The results are not promising and look like a normal distribution (see visualization part). Looking at specific examples we could not find any significance of either matches with high or low score. \n",
      "\n",
      "That's why we are now trying to investigate further if the change of topic similarity over time is significant. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#all_para_version_scores list of dicts with index = version, key = index of para and value similarity\n",
      "#all_mappings_with_changes list of dicts with index = version, key = index of para and value index of para in next version\n",
      "\n",
      "all_mappings_with_coherence_change = []\n",
      "for version_index, version_mapping in enumerate(all_mappings_with_changes):\n",
      "    curr_changes = {}\n",
      "    if version_index + 1 < len(all_para_version_scores_without_changes):\n",
      "        for key, value in version_mapping.iteritems():\n",
      "            coh_prev = all_para_version_scores[version_index][key]\n",
      "            coh_post = all_para_version_scores_without_changes[version_index + 1][value]\n",
      "            curr_changes[key] = coh_post - coh_prev\n",
      "    all_mappings_with_coherence_change.append(curr_changes)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result look again very distributed (visualization part) but you can clearly see outliers. Those seem to have an absolute value above .15. Let us look at examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_mappings_with_coherence_change_filtered = []\n",
      "for i, version in enumerate(all_mappings_with_coherence_change):\n",
      "    for key, value in version.iteritems():\n",
      "        if abs(value) > .15:\n",
      "            all_mappings_with_coherence_change_filtered.append((i, key, value))\n",
      "all_mappings_with_coherence_change_filtered.sort(key=lambda x: x[2])\n",
      "print \"There are\", len(all_mappings_with_coherence_change_filtered), \"non coherent paras\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#helper function to make the array readable by humans\n",
      "def show_coherence_change(index):\n",
      "    curr_change = all_mappings_with_coherence_change_filtered[index]\n",
      "    print \"The following mapped paragraphs had a topic similarity of\", curr_change[2]\n",
      "    print \"---------------------------------------------\"\n",
      "    print \"Old parat:\"\n",
      "    print current_paratexts[curr_change[0]][curr_change[1]]\n",
      "    print \"---------------------------------------------\"\n",
      "    print \"Changed para:\"\n",
      "    new = all_mappings_with_changes[curr_change[0]][curr_change[1]]\n",
      "    print current_paratexts[curr_change[0]+1][new]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_coherence_change(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All examples we look at have a significant change in length indicating that the change in length and the subsequent change of number of topics is more significant than the actual change of topic of the paragraph. This leads to the conclusion that while we found an algorithm to find significant changes of length it is not of value for us"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Predicting future changes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sp2\n",
      "versions = len(all_mappings)-1\n",
      "def get_future_mapping(version,index,future):\n",
      "    if version + future > versions:\n",
      "        return None\n",
      "    while future > 0:\n",
      "        index = all_mappings[version].get(index)\n",
      "        if not index:\n",
      "            return None\n",
      "        version += 1\n",
      "        future -=1\n",
      "    return index\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_future_mappings(version,index,future):\n",
      "    try: \n",
      "        a = current_paratexts[version][index]\n",
      "    except: \n",
      "        return None\n",
      "    \n",
      "    lst = [index]\n",
      "    while future > 0:\n",
      "        if version > versions:\n",
      "            return None\n",
      "        index = all_mappings[version].get(index)\n",
      "        if not index:\n",
      "            return None\n",
      "        lst.append(index)\n",
      "        version += 1\n",
      "        future -=1\n",
      "    return lst"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_backward_mapping(version,index,future):\n",
      "    try: \n",
      "        a = current_paratexts[version][index]\n",
      "    except: \n",
      "        return None\n",
      "    \n",
      "    lst = [index]\n",
      "    while future > 0:\n",
      "        i = None\n",
      "        for key, value in all_mappings[version-1].iteritems():\n",
      "            if value == index:\n",
      "                i = key\n",
      "        if not i:\n",
      "            return None\n",
      "        \n",
      "        lst.append(i)\n",
      "        version -= 1\n",
      "        future -=1\n",
      "    lst.reverse()\n",
      "    return lst[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_patterns_of_no_change(length, specificity):\n",
      "    dictionary = {}\n",
      "    mapping_changes = detect_mapping_changes(specificity)[0]\n",
      "    for v in xrange(len(all_mappings)-1):\n",
      "        for i in xrange(len(current_paratexts[v])):\n",
      "            mapped_index = i\n",
      "            current_version = v\n",
      "            changed = False\n",
      "            while not changed:\n",
      "                if current_version >= len(all_mappings):\n",
      "                    break\n",
      "                if mapped_index in mapping_changes[current_version].keys():\n",
      "                    changed = True\n",
      "                else:\n",
      "                    mapped_index = all_mappings[current_version].get(mapped_index)\n",
      "                    if mapped_index:\n",
      "                        current_version +=1\n",
      "                    else:\n",
      "                        break\n",
      "\n",
      "            if mapped_index:\n",
      "                if current_version - v >= length:\n",
      "                    if not (mapped_index, current_version) in dictionary.keys():\n",
      "                        dictionary[(mapped_index, current_version)] = (i,v)\n",
      "            \n",
      "    return dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#look at future values after each no change point for same paragraph\n",
      "def get_scores_after_no_change(specificity,length_of_no_change, graph=True, give_indices=False):\n",
      "    future_values = []\n",
      "    future_indices = []\n",
      "    for new,old in find_patterns_of_no_change(length_of_no_change,specificity).iteritems():\n",
      "        inew=new[0]\n",
      "        vnew=new[1]\n",
      "        iold = old[0]\n",
      "        vold = old[1]\n",
      "        indices = get_future_mappings(vnew,inew,length_of_no_change)\n",
      "        if indices:\n",
      "            array = []\n",
      "            for offset, paraindex in enumerate(indices):\n",
      "                if offset < len(indices)-1:\n",
      "                    array.append(Levenshtein.ratio(current_paratexts[vnew+offset][paraindex], current_paratexts[vnew+offset+1][indices[offset+1]]))\n",
      "            future_values.append(array)\n",
      "            if give_indices:\n",
      "                future_indices.append((inew, vnew))\n",
      "    \n",
      "    future_values = np.array(future_values)\n",
      "    if graph:\n",
      "        plt.plot(np.transpose(future_values))\n",
      "        remove_border()\n",
      "        plt.title(\"Further edits after end of a non-edit streak\")\n",
      "        plt.ylabel(\"similarity\")\n",
      "        plt.show()\n",
      "    if give_indices:\n",
      "        return future_values, future_indices\n",
      "    return future_values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = get_scores_after_no_change(.8,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#look at future values after each no change point for all paragraphs in version\n",
      "def get_all_scores_after_no_change(specificity,length_of_no_change, graph=True, give_indices=False):\n",
      "    future_values = []\n",
      "    future_indices = []\n",
      "    for new,old in find_patterns_of_no_change(length_of_no_change,specificity).iteritems():\n",
      "        inew=new[0]\n",
      "        vnew=new[1]\n",
      "        iold = old[0]\n",
      "        vold = old[1]\n",
      "        all_indices = [get_future_mappings(vnew,x,length_of_no_change) for x,y in enumerate(current_paratexts[vnew])]\n",
      "        \n",
      "        for k, indices in enumerate(all_indices):\n",
      "            if indices and not k == inew:\n",
      "                array = []\n",
      "                for offset, paraindex in enumerate(indices):\n",
      "                    if offset < len(indices)-1:\n",
      "                        array.append(Levenshtein.ratio(current_paratexts[vnew+offset][paraindex], current_paratexts[vnew+offset+1][indices[offset+1]]))\n",
      "                future_values.append(array)\n",
      "            if give_indices:\n",
      "                future_indices.append((inew, vnew))\n",
      "\n",
      "    future_values = np.array(future_values)\n",
      "    if graph:\n",
      "        plt.plot(np.transpose(future_values))\n",
      "        remove_border()\n",
      "        plt.title(\"Further edits after end of a non-edit streak\")\n",
      "        plt.ylabel(\"similarity\")\n",
      "        plt.show()\n",
      "    if give_indices:\n",
      "        return future_values, future_indices\n",
      "    return future_values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = get_all_scores_after_no_change(.9,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#look at future values after each no change point for all paragraphs in version\n",
      "def get_all_scores_before_after_no_change(specificity,length_of_no_change, graph=True, give_indices=False):\n",
      "    future_values = []\n",
      "    future_indices = []\n",
      "    for new,old in find_patterns_of_no_change(length_of_no_change,specificity).iteritems():\n",
      "        inew=new[0]\n",
      "        vnew=new[1]\n",
      "        iold = old[0]\n",
      "        vold = old[1]\n",
      "        all_indices = [get_future_mappings(vnew-length_of_no_change,x,length_of_no_change*2) for x,y in enumerate(current_paratexts[vnew])]\n",
      "\n",
      "        \n",
      "        for indices in all_indices:\n",
      "            if indices:\n",
      "                array = []\n",
      "                for offset, paraindex in enumerate(indices):\n",
      "                    if offset < len(indices)-1:\n",
      "                        array.append(Levenshtein.ratio(current_paratexts[vnew-length_of_no_change+offset][paraindex], current_paratexts[vnew-length_of_no_change+offset+1][indices[offset+1]]))\n",
      "                future_values.append(array)\n",
      "            if give_indices:\n",
      "                future_indices.append((inew, vnew))\n",
      "\n",
      "    future_values = np.array(future_values)\n",
      "    if graph:\n",
      "        plt.plot(np.transpose(future_values))\n",
      "        remove_border()\n",
      "        plt.title(\"Further edits after end of a non-edit streak\")\n",
      "        plt.ylabel(\"similarity\")\n",
      "        plt.show()\n",
      "    if give_indices:\n",
      "        return future_values, future_indices\n",
      "    return future_values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = get_all_scores_before_after_no_change(.8,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#look at future values after each no change point and before for all paragraphs\n",
      "def get_all_scores_before_after_no_change_avg(specificity,length_of_no_change, graph=True):\n",
      "    avgs = []\n",
      "    for new,old in find_patterns_of_no_change(length_of_no_change,specificity).iteritems():\n",
      "        inew=new[0]\n",
      "        vnew=new[1]\n",
      "        iold = old[0]\n",
      "        vold = old[1]\n",
      "        all_indices = [get_future_mappings(vnew-length_of_no_change,x,length_of_no_change*2) for x,y in enumerate(current_paratexts[vnew])]\n",
      "        \n",
      "        for k, indices in enumerate(all_indices):\n",
      "            if indices:\n",
      "                before = 0\n",
      "                after = 0\n",
      "                for offset, paraindex in enumerate(indices):\n",
      "                    if offset < len(indices)-1:\n",
      "                        if offset < length_of_no_change:\n",
      "                            before += (Levenshtein.ratio(current_paratexts[vnew-length_of_no_change+offset][paraindex], current_paratexts[vnew-length_of_no_change+offset+1][indices[offset+1]]))\n",
      "                        else:\n",
      "                            after += (Levenshtein.ratio(current_paratexts[vnew-length_of_no_change+offset][paraindex], current_paratexts[vnew-length_of_no_change+offset+1][indices[offset+1]]))\n",
      "                before = before/length_of_no_change\n",
      "                after = after/length_of_no_change\n",
      "                avgs.append(before-after)\n",
      "    if graph:\n",
      "        plt.hist(avgs, bins=21)\n",
      "        plt.show()\n",
      "    \n",
      "    return avgs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = get_all_scores_before_after_no_change_avg(.9,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sp3\n",
      "#look at future values after each no change point for all paragraphs in version\n",
      "def get_neighbor_scores_after_no_change(specificity,length_of_no_change, graph=True, give_indices=False):\n",
      "    future_values = []\n",
      "    future_indices = []\n",
      "    for new,old in find_patterns_of_no_change(length_of_no_change,specificity).iteritems():\n",
      "        inew=new[0]\n",
      "        vnew=new[1]\n",
      "        iold = old[0]\n",
      "        vold = old[1]\n",
      "        all_indices = [get_future_mappings(vnew,inew+1,length_of_no_change), get_future_mappings(vnew,inew-1,length_of_no_change)]\n",
      "        \n",
      "        for indices in all_indices:\n",
      "            if indices:\n",
      "                array = []\n",
      "                for offset, paraindex in enumerate(indices):\n",
      "                    if offset < len(indices)-1:\n",
      "                        array.append(Levenshtein.ratio(current_paratexts[vnew+offset][paraindex], current_paratexts[vnew+offset+1][indices[offset+1]]))\n",
      "                future_values.append(array)\n",
      "            if give_indices:\n",
      "                future_indices.append((inew, vnew))\n",
      "\n",
      "    future_values = np.array(future_values)\n",
      "    if graph:\n",
      "        plt.plot(np.transpose(future_values))\n",
      "        remove_border()\n",
      "        plt.title(\"Further edits after end of a non-edit streak\")\n",
      "        plt.ylabel(\"similarity\")\n",
      "        plt.show()\n",
      "    if give_indices:\n",
      "        return future_values, future_indices\n",
      "    \n",
      "    return future_values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = get_neighbor_scores_after_no_change(.9,10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#look at future values after each no change point for all paragraphs in version\n",
      "def get_neighbor_scores_before_after_no_change(specificity,length_of_no_change, graph=True, give_indices=False):\n",
      "    future_values = []\n",
      "    future_indices = []\n",
      "    for new,old in find_patterns_of_no_change(length_of_no_change,specificity).iteritems():\n",
      "        inew=new[0]\n",
      "        vnew=new[1]\n",
      "        iold = old[0]\n",
      "        vold = old[1]\n",
      "        back1 = get_backward_mapping(vnew,inew+1, length_of_no_change)\n",
      "        back2 = get_backward_mapping(vnew,inew-1, length_of_no_change)\n",
      "        all_indices = [get_future_mappings(vnew-length_of_no_change,back1,length_of_no_change*2), get_future_mappings(vnew-length_of_no_change,back2,length_of_no_change*2)]\n",
      "        \n",
      "        for indices in all_indices:\n",
      "            if indices:\n",
      "                array = []\n",
      "                for offset, paraindex in enumerate(indices):\n",
      "                    if offset < len(indices)-1:\n",
      "                        array.append(Levenshtein.ratio(current_paratexts[vnew-length_of_no_change+offset][paraindex], current_paratexts[vnew-length_of_no_change+offset+1][indices[offset+1]]))\n",
      "                future_values.append(array)\n",
      "            if give_indices:\n",
      "                future_indices.append((inew, vnew))\n",
      "\n",
      "    future_values = np.array(future_values)\n",
      "    if graph:\n",
      "        plt.plot(np.transpose(future_values))\n",
      "        remove_border()\n",
      "        plt.title(\"Further edits after end of a non-edit streak\")\n",
      "        plt.ylabel(\"similarity\")\n",
      "        plt.show()\n",
      "    if give_indices:\n",
      "        return future_values, future_indices\n",
      "    \n",
      "    return future_values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = get_neighbor_scores_before_after_no_change(.9,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#look at future values after each no change point for all paragraphs in version\n",
      "def get_neighbor_scores_before_after_no_change_avg(specificity,length_of_no_change, graph=True):\n",
      "    avgs = []\n",
      "    for new,old in find_patterns_of_no_change(length_of_no_change,specificity).iteritems():\n",
      "        inew=new[0]\n",
      "        vnew=new[1]\n",
      "        iold = old[0]\n",
      "        vold = old[1]\n",
      "        back1 = get_backward_mapping(vnew,inew+1, length_of_no_change)\n",
      "        back2 = get_backward_mapping(vnew,inew-1, length_of_no_change)\n",
      "        all_indices = [get_future_mappings(vnew-length_of_no_change,back1,length_of_no_change*2), get_future_mappings(vnew-length_of_no_change,back2,length_of_no_change*2)]\n",
      "                \n",
      "        for indices in all_indices:\n",
      "            if indices:\n",
      "                before = 0\n",
      "                after = 0\n",
      "                for offset, paraindex in enumerate(indices):\n",
      "                    if offset < len(indices)-1:\n",
      "                        if offset < length_of_no_change:\n",
      "                            before += (Levenshtein.ratio(current_paratexts[vnew-length_of_no_change+offset][paraindex], current_paratexts[vnew-length_of_no_change+offset+1][indices[offset+1]]))\n",
      "                        else:\n",
      "                            after += (Levenshtein.ratio(current_paratexts[vnew-length_of_no_change+offset][paraindex], current_paratexts[vnew-length_of_no_change+offset+1][indices[offset+1]]))\n",
      "                before = before/length_of_no_change\n",
      "                after = after/length_of_no_change\n",
      "                avgs.append(before-after)\n",
      "    if graph:\n",
      "        plt.hist(avgs, bins=21)\n",
      "        plt.show()\n",
      "    \n",
      "    return avgs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = get_neighbor_scores_before_after_no_change_avg(.9,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sp4\n",
      "def plot_scatter_for_neighbors(specificity, length_of_no_change, graph=False):\n",
      "    initial_change_future_values, initial_change_future_values_indices = get_scores_after_no_change(specificity,length_of_no_change, graph=False, give_indices=True)\n",
      "    neighbor_change_future_values, neighbor_change_future_values_indices = get_neighbor_scores_after_no_change(specificity,length_of_no_change, graph=False, give_indices=True)\n",
      "\n",
      "\n",
      "    initial_change_value = [x[0] for x in initial_change_future_values]\n",
      "    neighbor_change_future_average = [np.mean(x) for x in neighbor_change_future_values]\n",
      "\n",
      "    x_scatter_initial_change = []\n",
      "    y_scatter_avg_change_neighbors = []\n",
      "    for key, value in enumerate(neighbor_change_future_average):\n",
      "        index_to_search_for = neighbor_change_future_values_indices[key]\n",
      "        for a, x in enumerate(initial_change_future_values_indices):\n",
      "            if x == index_to_search_for:\n",
      "                x_scatter_initial_change.append(initial_change_value[a])\n",
      "                y_scatter_avg_change_neighbors.append(value)\n",
      "    if graph:\n",
      "        plt.scatter(x_scatter_initial_change, y_scatter_avg_change_neighbors)\n",
      "        plt.title(\"initial change of paragraph compared to average change of neighbors\")\n",
      "        plt.ylabel(\"avg change of neighbor\")\n",
      "        plt.xlabel(\"initial change of paragaraph\")\n",
      "    return (x_scatter_initial_change, y_scatter_avg_change_neighbors)\n",
      "\n",
      "test = plot_scatter_for_neighbors(.9, 10, graph=True)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_scatter_for_all(specificity, length_of_no_change, graph=False):\n",
      "    initial_change_future_values, initial_change_future_values_indices = get_scores_after_no_change(specificity,length_of_no_change, graph=False, give_indices=True)\n",
      "    all_change_future_values, all_change_future_values_indices = get_all_scores_after_no_change(specificity,length_of_no_change, graph=False, give_indices=True)\n",
      "\n",
      "\n",
      "    initial_change_value = [x[0] for x in initial_change_future_values]\n",
      "    all_change_future_average = [np.mean(x) if np.mean(x) <= 1 else 0 for x in all_change_future_values]\n",
      "\n",
      "    x_scatter_initial_change = []\n",
      "    y_scatter_avg_change_all = []\n",
      "    for key, value in enumerate(all_change_future_average):\n",
      "        if value:\n",
      "            index_to_search_for = all_change_future_values_indices[key]\n",
      "            for a, x in enumerate(initial_change_future_values_indices):\n",
      "                if x == index_to_search_for:\n",
      "                    if value != initial_change_value[a]:\n",
      "                        x_scatter_initial_change.append(initial_change_value[a])\n",
      "                        y_scatter_avg_change_all.append(value)\n",
      "\n",
      "    if graph:\n",
      "        plt.scatter(x_scatter_initial_change, y_scatter_avg_change_all, alpha=.2)\n",
      "        plt.title(\"initial change of paragraph compared to average change of all paragraphs\")\n",
      "        plt.ylabel(\"avg change of all others\")\n",
      "        plt.xlabel(\"initial change of paragaraph\")\n",
      "        plt.xlim([.55, .9])\n",
      "\n",
      "    return x_scatter_initial_change, y_scatter_avg_change_all\n",
      "\n",
      "test = plot_scatter_for_all(.9, 10, True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##linked paragraph changes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sp5\n",
      "def get_backward_mapping_indefinite(version,index,min_past):\n",
      "    try: \n",
      "        a = current_paratexts[version][index]\n",
      "    except: \n",
      "        return None\n",
      "    lst = [index]\n",
      "    broken = False\n",
      "    past = 0\n",
      "    while not broken:\n",
      "        i = None\n",
      "        if version - 1 >= 0:\n",
      "            for key, value in all_mappings[version-1].iteritems():\n",
      "                if value == index:\n",
      "                    i = key\n",
      "        if not i:\n",
      "            broken = True\n",
      "        else:\n",
      "            lst.append(i)\n",
      "            version -= 1\n",
      "            past +=1\n",
      "    if past > min_past:\n",
      "        lst.reverse()\n",
      "        return lst\n",
      "    return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_future_mappings_indefinite(version,index,min_future):\n",
      "    try: \n",
      "        a = current_paratexts[version][index]\n",
      "    except: \n",
      "        return None\n",
      "    \n",
      "    lst = [index]\n",
      "    broken = False\n",
      "    future = 0\n",
      "    while not broken:\n",
      "        i = None\n",
      "        if version < versions:\n",
      "        \n",
      "            for key, value in all_mappings[version+1].iteritems():\n",
      "                if value == index:\n",
      "                    i = key\n",
      "        if not i:\n",
      "            broken = True\n",
      "        else:\n",
      "            lst.append(i)\n",
      "            version += 1\n",
      "            future +=1\n",
      "    if future > min_future:\n",
      "        return lst\n",
      "    return None\n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_significant_changes(threshold):\n",
      "    significant_changes = detect_mapping_changes(threshold)[0]\n",
      "    significant_dictionary = {} \n",
      "    for key, value in enumerate(significant_changes):\n",
      "        if value:\n",
      "            significant_dictionary[key] = value.keys()\n",
      "    return significant_dictionary\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "find_significant_changes(.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_all_backtracks_of_linkings(threshold, min_past):\n",
      "    sig_changes = find_significant_changes(threshold)\n",
      "    all_backtracks = {}\n",
      "    for a in sig_changes.keys():\n",
      "        sig_change_tracked = False\n",
      "        all_version_backtracks = {}\n",
      "        for i in xrange(len(current_paratexts[a])):\n",
      "            bw = get_backward_mapping_indefinite(a, i, min_past)\n",
      "            if bw:\n",
      "                if i in sig_changes[a]:\n",
      "                    sig_change_tracked = True\n",
      "                all_version_backtracks[i] = bw\n",
      "        if all_version_backtracks and len(all_version_backtracks) > 1 and sig_change_tracked:\n",
      "            all_backtracks[a] = all_version_backtracks\n",
      "    return all_backtracks, sig_changes\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print find_all_backtracks_of_linkings(.5, 10)[0].keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_backtrack_scores(threshold, min_past):\n",
      "    backtrack, sig_changes = find_all_backtracks_of_linkings(threshold, min_past)\n",
      "    all_backtrack_scores = {}\n",
      "    for version, value in backtrack.iteritems():\n",
      "        backtrack_version_scores = {}\n",
      "        for last_index, past_indices in value.iteritems():\n",
      "            score_array = []\n",
      "            for i, index in enumerate(past_indices):\n",
      "                if i < len(past_indices)-1:\n",
      "                    l_ratio = Levenshtein.ratio(current_paratexts[version-len(past_indices)+i+1][index], current_paratexts[version-len(past_indices)+i+2][past_indices[i+1]])\n",
      "                else:\n",
      "                    l_ratio = Levenshtein.ratio(current_paratexts[version-1][index], current_paratexts[version][last_index])\n",
      "                score_array.append(l_ratio)\n",
      "            backtrack_version_scores[last_index] = score_array\n",
      "        all_backtrack_scores[version] = backtrack_version_scores\n",
      "    return backtrack, sig_changes, all_backtrack_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = find_backtrack_scores(.5, 10)\n",
      "print len(test[0]), len(test[1]), len(test[2])\n",
      "print test[1]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#if sig change happens, detect which paragraphs are linked in that version\n",
      "def find_para_linked_to_sig_change(sig_change_threshold, min_past,change_occured_threshold, link_threshold):    \n",
      "    backtrack, sig_changes, all_backtrack_scores = find_backtrack_scores(sig_change_threshold, min_past)\n",
      "    linked_paragraphs_by_version = {}\n",
      "    for version, indices in sig_changes.iteritems():\n",
      "        if version in all_backtrack_scores.keys():\n",
      "            list_of_links = []\n",
      "            for index in indices:\n",
      "                if not index in all_backtrack_scores[version].keys():\n",
      "                    break\n",
      "                iscores = all_backtrack_scores[version][index]\n",
      "                iscores = [0 if x < change_occured_threshold else 1 for x in iscores]\n",
      "                for compare,cscores in all_backtrack_scores[version].iteritems():\n",
      "                    if not compare == index:\n",
      "                        iters = min(len(iscores),len(cscores))\n",
      "                        cscores = [0 if x < change_occured_threshold else 1 for x in cscores]\n",
      "                        new_iscores = iscores[-iters:]\n",
      "                        cscores = cscores[-iters:]\n",
      "                        if not len(new_iscores) == len(cscores):\n",
      "                            print \"not the same length, oh no\"\n",
      "                        iscores_changes = len(new_iscores) - sum(new_iscores)\n",
      "                        count = 0\n",
      "                        for j, item in enumerate(cscores):\n",
      "                            if item == 0 and new_iscores[j] == 0:\n",
      "                                count +=1\n",
      "                        if iscores_changes == 0:\n",
      "                            avg = 0\n",
      "                        else:\n",
      "                            avg = float(count)/iscores_changes\n",
      "                        if avg > link_threshold:\n",
      "                            '''if compare < index:\n",
      "                                if not (compare,index) in list_of_links:\n",
      "                                    list_of_links.append((compare,index))\n",
      "                            else:\n",
      "                                if not (index,compare) in list_of_links:\n",
      "                                    list_of_links.append((index,compare))'''\n",
      "                            list_of_links.append((index,compare))\n",
      "\n",
      "            if list_of_links:\n",
      "                linked_paragraphs_by_version[version] = list_of_links\n",
      "    return linked_paragraphs_by_version"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "find_para_linked_to_sig_change(.8,5,.9,.4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# determine if linked paras change together in the future\n",
      "def find_future_of_linked_paras(sig_change_threshold, min_past,change_occured_threshold, link_threshold, n_into_future_min, n_into_future_max):    \n",
      "    #find sig changes linked paras to validate in future\n",
      "    linked_paras_with_sig_change = find_para_linked_to_sig_change(sig_change_threshold,min_past,change_occured_threshold,link_threshold)\n",
      "    \n",
      "    total_prev_links = 0\n",
      "    found_link = 0\n",
      "    looked_at = 0\n",
      "    #construct new dictionary to validate\n",
      "    validation_dictionary = copy.deepcopy(linked_paras_with_sig_change)\n",
      "    for v in linked_paras_with_sig_change:\n",
      "        for i,link in enumerate(linked_paras_with_sig_change[v]):\n",
      "            total_prev_links += 1\n",
      "            t = None\n",
      "            get_fm = get_future_mappings_indefinite(v,link[0],n_into_future_min)\n",
      "            get_fm2 = get_future_mappings_indefinite(v,link[1],n_into_future_min)\n",
      "            if not (get_fm and get_fm2):\n",
      "                validation_dictionary[v][i] = (linked_paras_with_sig_change[v][i],False)\n",
      "            else:\n",
      "                looked_at+=1\n",
      "                min_len = min(len(get_fm), len(get_fm2), n_into_future_max+1)\n",
      "                get_fm = get_fm[:min_len]\n",
      "                get_fm2 = get_fm2[:min_len]\n",
      "                t = (get_fm[-1],get_fm2[-1])\n",
      "                \n",
      "                linked = False\n",
      "                #calculate if still linked (2 dimensional??)\n",
      "                change_scores1 = []\n",
      "                change_scores2 = []\n",
      "                #version is v\n",
      "                #get_fmX is indices\n",
      "                \n",
      "                \n",
      "                for t, para_index in enumerate(get_fm):\n",
      "                    if t< len(get_fm)-1: \n",
      "                        change_scores1.append(Levenshtein.ratio(current_paratexts[v+t][para_index], current_paratexts[v+t+1][get_fm[t+1]]))\n",
      "                for t, para_index in enumerate(get_fm2):\n",
      "                    if t< len(get_fm2)-1: \n",
      "                        change_scores2.append(Levenshtein.ratio(current_paratexts[v+t][para_index], current_paratexts[v+t+1][get_fm2[t+1]]))\n",
      "                \n",
      "                num_first_changes = 0\n",
      "                for change in change_scores1:\n",
      "                    if change < change_occured_threshold:\n",
      "                        num_first_changes += 1\n",
      "                num_second_changes = 0\n",
      "                for t, change in enumerate(change_scores2):\n",
      "                    if change < change_occured_threshold and change_scores1[t] < change_occured_threshold:\n",
      "                            num_second_changes += 1\n",
      "                avg = 0\n",
      "                if num_first_changes == 0 and num_second_changes == 0:\n",
      "                    avg = 1.0\n",
      "                elif num_first_changes > 0:\n",
      "                    avg = float(num_second_changes)/num_first_changes\n",
      "                \n",
      "                        \n",
      "                #only do from current verision to this n versions away\n",
      "                \n",
      "                if avg > link_threshold:\n",
      "                    validation_dictionary[v][i] = (linked_paras_with_sig_change[v][i],avg)\n",
      "                    found_link +=1\n",
      "                else: \n",
      "                    validation_dictionary[v][i] = (linked_paras_with_sig_change[v][i],False, avg)\n",
      "    \n",
      "    \n",
      "    return validation_dictionary, (found_link, looked_at, total_prev_links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = find_future_of_linked_paras(.8,5,.8,.4,4,13)\n",
      "if test[1][1]>0:\n",
      "    print \"out of\", test[1][2], \"links found,\", test[1][1], \"were still trackable.\", test[1][0], \"were still linked. This is an accuracy of\", test[1][0]/float(test[1][1])*100, \"%.\"\n",
      "\n",
      "print \"=======================\"\n",
      "\n",
      "test[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''def drange(start, stop, step):\n",
      "    r = start\n",
      "    while r < stop: \n",
      "        yield r\n",
      "        r += step\n",
      "        \n",
      "        \n",
      "def find_best_paras_for_future_of_links():\n",
      "    #.8,5,.8,.4,5,10\n",
      "    #sig_change_threshold = drange(.5, 1.0, .05)\n",
      "    sig_change_threshold = [.8]\n",
      "    min_past = [5] #xrange(4, 11, 2)\n",
      "    #change_occured_threshold = drange(.74, 1.0, .02)\n",
      "    change_occured_threshold = [.8]\n",
      "    link_threshold = [.4]\n",
      "    #link_threshold = drange(.3, 1.0, .1)\n",
      "    n_into_future_min = xrange(1, 10, 1)\n",
      "    n_into_future_max = xrange(5, 20, 1)\n",
      "    \n",
      "    eval_data = []\n",
      "    for a in sig_change_threshold:\n",
      "        for b in min_past:\n",
      "            for c in change_occured_threshold:\n",
      "                for d in link_threshold:\n",
      "                    for e in n_into_future_min:\n",
      "                        for f in n_into_future_max:\n",
      "                            data_from_func_to_eval = find_future_of_linked_paras(a, b, c, d, e, f)[1]\n",
      "                            eval_data.append(((a, b, c, d, e, f), data_from_func_to_eval))\n",
      "    return eval_data\n",
      "\n",
      "link_into_future_parameter_data = find_best_paras_for_future_of_links()\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''max_data_values = 0\n",
      "max_data_ratio = 0\n",
      "\n",
      "max_found = 0\n",
      "x_values = []\n",
      "y_values = []\n",
      "for link in link_into_future_parameter_data:\n",
      "    print link\n",
      "    if link[1][1] > 0:\n",
      "        data_ratio = link[1][0]/float(link[1][1])*100\n",
      "        if data_ratio > max_data_ratio:\n",
      "            max_data_values = link\n",
      "            max_data_ratio = data_ratio\n",
      "            \n",
      "        if link[1][2] > max_found:\n",
      "            max_found = link[1][2]\n",
      "            \n",
      "        x_values.append(link[1][2])\n",
      "        y_values.append(data_ratio)\n",
      "            \n",
      "plt.hist2d(x_values,y_values, bins=40, cmap=cm.YlGn);\n",
      "remove_border()\n",
      "\n",
      "plt.title(\"total links found versus predicted percentage\")\n",
      "plt.xlabel(\"total links\")\n",
      "plt.ylabel(\"percentage of true prositives\")\n",
      "plt.colorbar()\n",
      "plt.show()\n",
      "\n",
      "print max_data_ratio\n",
      "print max_data_values\n",
      "print max_found '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cell for storing pickles sp6\n",
      "if store_pickle:\n",
      "    obj = prediction_pickle()\n",
      "    sig_change_threshold = .8\n",
      "    length_of_no_change_threshold = 10\n",
      "    min_past = 5\n",
      "    change_occured_threshold = .8\n",
      "    link_threshold = .4\n",
      "    n_into_future_min = 4\n",
      "    n_into_future_max = 10\n",
      "    versions = len(all_mappings) -1\n",
      "    \n",
      "    obj.versions = versions\n",
      "    obj.same_after = get_scores_after_no_change(sig_change_threshold, length_of_no_change_threshold)\n",
      "    obj.all_after = get_all_scores_after_no_change(sig_change_threshold, length_of_no_change_threshold)\n",
      "    obj.all_before_after = get_all_scores_before_after_no_change(specificity = sig_change_threshold,length_of_no_change = length_of_no_change_threshold, graph=False, give_indices=False)\n",
      "    obj.all_avg_before_after = get_all_scores_before_after_no_change_avg(specificity= sig_change_threshold,length_of_no_change = length_of_no_change_threshold, graph=False)\n",
      "    obj.neighbor_after = get_neighbor_scores_after_no_change(specificity = sig_change_threshold,length_of_no_change = length_of_no_change_threshold, graph=False, give_indices=True)\n",
      "    obj.neighbor_before_after_values_indices = get_neighbor_scores_before_after_no_change(specificity = sig_change_threshold,length_of_no_change = length_of_no_change_threshold, graph=False, give_indices=True)\n",
      "    obj.neighbor_avg_before_after = get_neighbor_scores_before_after_no_change_avg(specificity = sig_change_threshold,length_of_no_change = length_of_no_change_threshold, graph=False)\n",
      "    obj.xscatter_initial_n, obj.yscatter_neighbor = plot_scatter_for_neighbors(specificity = sig_change_threshold, length_of_no_change = length_of_no_change_threshold, graph=False)\n",
      "    obj.xscatter_initial_a, obj.yscatter_all = plot_scatter_for_all(specificity = sig_change_threshold, length_of_no_change = length_of_no_change_threshold, graph=False)\n",
      "    \n",
      "    prediction_file = os.path.join(os.getcwd(), \"predictions\", pickle_file_name)\n",
      "    pkl_file = open(prediction_file, 'wb')\n",
      "    cPickle.dump(obj, pkl_file)\n",
      "    pkl_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- if one para changes, does it trigger changes in neighbor/same paragraphs in the next versions \u2713\n",
      "    - trigger changes in a paragraph that isn\u2019t neighboring but is linked with topic similarity or by previous changes that always occur together (is the paragraph linked to other paragraph\u2019s being edited?) \u2713\n",
      "- intensity of the edit -  how much was the paragraph edited in the past\n",
      "- interaction based on authors - who edits what and who is responsible for which section, is the intro and conclusion important for both?\n",
      "- actions that lead to deletions or additions of paragraphs\n",
      "    - rapid change in neighbor?\n",
      "    - change in entire version\u2019s topic?\n",
      "- detect bad edits by looking if something gets changed back? then look for unique features of this change to try to predict\n",
      "- if change para 8, then next revision changes para 2 and the similarity between 8 and 2 increases, then we know that the changes were related\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###To DO:\n",
      "\n",
      "- look whether a rapid change of topic triggers further edits\n",
      "    - look whether a rapid change in one paragraph -> paragraph deleted in future versions\n",
      "    - look whether rapid change in one paragraph -> edits in neighboring paragraphs (good change)\n",
      "    - look whether rapid change in one paragraph -> rapid change back (bad change)\n",
      "    - look at what precedes added paragrpahs or deleted paragraphs\n",
      "        - rapid change in one neighbor paragraph\n",
      "        - change in version topic\n",
      "-  if change para 8, then next revision changes para 2 and the similarity between 8 and 2 increases, then we know that the changes were related\n",
      "- look at interaction between authors\n",
      "    - prepare by looking at comments and compute pickles\n",
      "- create a LSI model based on wikipedia - look into amazon to do it for us. Put file on internet!\n",
      "- vis idea: small squares (paras) stacked on x axis (versions) in color corresponsing to match/topic relation OR line graph with connections\n",
      "- coherence algorithm Idea:\n",
      "compare a paragraph to predecessor/successor paragraphs in two neighboring versions and see whether this number went up or down, for comparism use LSI again\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## visualisation of current variables\n",
      "\n",
      "###Topic similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to look at the distribution of topic similarity between matched paragraphs. We only show not perfectly aligned paragraphs (only the ones where the text changed).\n",
      "Each bin represents 4% and there are 95 matches lower than .8"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_of_values = []\n",
      "for v in all_mappings_changed_scores:\n",
      "    for key, value in v.iteritems():\n",
      "        list_of_values.append(value)\n",
      "plt.hist([a for a in list_of_values if a < .98], bins=25)\n",
      "plt.title('topic similarity between mappings')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_of_values = []\n",
      "for v in all_para_version_scores_filtered:\n",
      "    for key, value in v.iteritems():\n",
      "        list_of_values.append(value)\n",
      "plt.hist([a for a in list_of_values if a < .98], bins=25)\n",
      "plt.title('topic similarity between paragraph and whole text')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_of_values = []\n",
      "for v in all_mappings_with_coherence_change:\n",
      "    for key, value in v.iteritems():\n",
      "        list_of_values.append(value)\n",
      "plt.hist([a for a in list_of_values if a < .98], bins=25, log=True)\n",
      "plt.title('change of topic similarity para-text between changes (change in coherence)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###similarity of different similarity metrics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we had several possible similarity metrics most of which are based on the levenshtein distance we wanted to compare them. The following plots show the distributions of running them over all paragraphs of all versions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Only need to get run the first time a pickle is evaluated. Can take upwards of 20 minutes\n",
      "'''\n",
      "max_lev_dists = generate_all_ratios()\n",
      "#roughly 90 sec\n",
      "max_sm_dists = generate_all_ratios(how=sequence_sim)\n",
      "#roughly 150 sec\n",
      "max_cosine_list = generate_all_ratios(how=cosine_sim)\n",
      "#roughly 5 minutes\n",
      "max_jelly_dists = generate_all_ratios(how=jelly_sim)\n",
      "#roughly 260 sec\n",
      "max_lists = store_max_lists(max_lev_dists, max_sm_dists, max_cosine_list, max_jelly_dists)\n",
      "max_list_file = os.path.join(os.getcwd(), \"max_lists\", pickle_file_name)\n",
      "pkl_file = open(max_list_file, 'wb')\n",
      "cPickle.dump(max_lists, pkl_file)\n",
      "pkl_file.close()\n",
      "'''\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#if we already computed the lists we can just load it from hard drive\n",
      "\n",
      "max_lists = get_pickle(pickle_file_name, folder=\"max_lists\")\n",
      "max_lev_dists = max_lists.lev\n",
      "max_sm_dists = max_lists.sm\n",
      "max_cosine_list = max_lists.cosine\n",
      "max_jelly_dists = max_lists.jelly\n",
      "'''\n",
      "pass\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist([a for a in max_lev_dists if a <.98], bins=100, log=False)\n",
      "plt.title(\"levenshtein distances\")\n",
      "plt.ylim(0, 70)\n",
      "plt.show()\n",
      "plt.hist([a for a in max_sm_dists if a <.98], bins=100, log=False)\n",
      "plt.title(\"SequenceMatcher distances\")\n",
      "plt.ylim(0, 70)\n",
      "plt.show()\n",
      "plt.hist([a for a in max_jelly_dists if a <.98], bins=100, log=False)\n",
      "plt.title(\"Jellyfish distances\")\n",
      "plt.ylim(0, 70)\n",
      "plt.show()\n",
      "plt.hist([a for a in max_cosine_list if a <.98], bins=100, log=False)\n",
      "plt.title(\"cosine similarities\")\n",
      "plt.ylim(0, 70)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###development of length of an article"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#plot lengths of articles\n",
      "plt.plot(map(len, current_texts[0:]))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###development of similarity ratio and distance over time"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#show all Levenshtein values for neighboring texts\n",
      "L_ratios = []\n",
      "L_distances = []\n",
      "for i in xrange(len(current_texts)):\n",
      "    if i < len(current_texts)-1:\n",
      "        L_ratios.append(Levenshtein.ratio(current_texts[i], current_texts[i+1]))\n",
      "        L_distances.append(Levenshtein.distance(current_texts[i], current_texts[i+1]))\n",
      "        \n",
      "plt.plot(L_ratios)\n",
      "plt.title(\"Levenshtein ratios\")\n",
      "plt.show()\n",
      "plt.plot(L_distances)\n",
      "plt.title(\"Levenshtein distances\")\n",
      "plt.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###comparism of two versions based on similarity between their paragraphs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_2d_map(lev_dists):\n",
      "    x_values = []\n",
      "    y_values = []\n",
      "    z_values = []\n",
      "    for i, v in enumerate(lev_dists):\n",
      "        for j, w in enumerate(v):\n",
      "            x_values.append(i)\n",
      "            y_values.append(j)\n",
      "            z_values.append(w)\n",
      "    grid = np.reshape(z_values, (max(x_values)+1, max(y_values)+1))\n",
      "    plt.imshow(grid, interpolation='nearest', cmap=cm.YlGn)\n",
      "    plt.title(\"Comparism of lev ratios\")\n",
      "    plt.xlabel(\"Old paragraphs\")\n",
      "    plt.ylabel(\"New paragraphs\")\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "plot_2d_map(generate_ratios(current_paratexts[5], current_paratexts[7], reverse=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###how does a paragraph fit into a text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def visualize_mappings():\n",
      "    height = len(max(current_paratexts, key=lambda x: len(x)))\n",
      "    width = len(current_paratexts)\n",
      "    \n",
      "    a = gca()\n",
      "    for x, array in enumerate(current_paratexts):\n",
      "        if x<len(current_paratexts)-1:\n",
      "            for y in xrange(len(array)):\n",
      "                a.add_patch(Rectangle((0+x,0+y),1,1, color=cm.Greens(all_para_version_scores_without_changes[x][y])))\n",
      "            \n",
      "    a.set_xlim([0,width-1])\n",
      "    a.set_ylim([0,height])\n",
      "    plt.title(\"how does a paragraph fit into the text (change over time)\")\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "visualize_mappings()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print current_paratexts[8][-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Experimental code"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}