(iversion
Page
p1
(dp2
S'revisions'
p3
(lp4
(iversion
Version
p5
(dp6
S'date'
p7
cdatetime
datetime
p8
(S'\x07\xd5\x08\x1a\x12/\x18\x00\x00\x00'
tRp9
sS'text'
p10
VThis paper defines a new multi-agent decision problem, the ``Single Agent in a Team Decision" ({\u005cem SATD}) problem, which may be described informally as follows: An individual collaborating in a multi-agent team obtains new information, unanticipated at planning time.  This (single) agent has incomplete knowledge of others' plans.  It must decide whether to communicate this new information to its teammates, and if so, to whom, and at what time.  SATD differs from previously studied multi-agent communications problems in that it does not assume complete knowledge of other agents' plans or policies nor that all observations are knowable in advance.  It assumes instead that agents have some knowledge of each other's intentions and plans which can be used to reason about information sharing decisions.  \u000aSATD arises from the ways in which effective teamwork decomposes complex activities into constituent tasks and delegates responsibility for those tasks to team members with appropriate expertise and capabilities.  Human teammates typically make only general plans and allocate tasks at a high level of abstraction. They do not necessarily know each other's plans  nor consider together all contingencies of all possible plans for doing those tasks. Their cognitive load is lowered significantly as a result. \u000aTo make appropriate decisions about sharing information, they must reason  with only uncertain knowledge of their teammates' plans. Agents participating in mixed networks comprising  humans and computer agents or supporting people in team settings also must be capable of such reasoning.  The SATD problem can also arise in purely computer-agent teamwork settings. For example, in ad hoc teamwork~\u005ccite{stone2010ad}, an agent joining an existing team only after planning time  lacks significant information about other agents' plans, but might still need to reason about which observations to share with teammates. \u000aWe are investigating SATD in the context of developing computer agents to support care teams for children with complex conditions.  Agents able to support health-care teams by identifying what information to share, with whom and when have the potential to substantially improve health outcomes. Care teams for children with complex conditions typically involve many providers -- a primary care provider, specialists, therapists, and non-medical care givers. The care team defines a high-level care plan that describes the main care goals, but there is no centralized planning mechanism that generates a complete plan for the team or that can ensure coordination. Caregivers are unaware of their collaborators' complete plans, yet their individual plans often interact. Communicating relevant information among team members is crucial for care to be coordinated and effective, but doing so is costly and often insufficient in practice.\u000aTo address SATD, the paper proposes a novel, integrated Belief-Desire-Intention (BDI) and decision-theoretic (DT) representation that\u000abuilds on the strengths of each approach.  \u000aIn particular, our approach integrates the Probabilistic Recipe Trees (PRT) representation of an agent's beliefs about another agent's plans~\u005ccite{kamar2009incorporating} with a Markov Decision Process (MDP) to support a collaborating group in their execution of a plan. We call this integrated representation  MDP-PRT. \u000aWe evaluated an agent using MDP-PRT to solve the SATD problem in an abstract setting. Results show that it outperforms the inform algorithm proposed by Kamar et al.~\u005cshortcite{kamar2009incorporating}. In addition, we compared the agent's performance with that of \u000aDec-POMDP policy that was informed about all possible observations.\u000aThe MDP-PRT agent obtains results close to those obtained by this Dec-POMDP policy despite lacking a  coordinated policy that considers all possible observations.\u000aThe paper makes three contributions. First, it formally defines  the SATD communication problem\u000a and contrasts it with previously studied communication problems in multi-agent settings. Second, it proposes a new representation (MDP-PRT)  that enables agents to reason about and solve the SATD communication problem. Third, it demonstrates the usefulness of this representation and analyzes the effect of agents' uncertainties and of communication cost on the performance of a team.\u000aIn this section, we formally define the SATD problem. SATD arises in the context of a group activity of a team of agents.  We assume the group's plan meets the SharedPlans specification for collaborative action~\u005ccite{grosz1996collaborative}.  Two particular properties of SharedPlans are important:  plans, which decompose into constituent tasks, may be only partially specified and agents do not know the details of constituent tasks for which they are not responsible.\u000a An instance of the SATD problem  is represented by a tuple $\u005cbf{\u005clangle a_i, A_{-i}, b_{SP}, V, o^*, \u005cvarphi_{comm}, C \u005crangle}$:\u000aSATD is the problem of $a_i$ determining whether to communicate $o^*$ to agents in  $A_{-i}$ and if so, at what time. \u000aIt has two components: determining which agents are candidates for receiving the information and deciding whether, and when, to send information to (some or all) candidates.  This paper focuses on the constituent problem of deciding whether and when to send information. \u000aTheories of teamwork and collaboration~\u005ccite{grosz1996collaborative,cohen1990intention,sonenberg1992planned} emphasize the key role of communication in teamwork. BDI approaches to multi-agent planning, e.g. STEAM~\u005ccite{tambe1997agent}, often base their communication mechanisms on these theories. BDI approaches typically do not reason about uncertainty and utilities~\u005ccite{pynadath2002communicative}. \u000aPrior work on decision theoretic approaches to multi-agent communication can generally be classified into two types: approaches that reason about communication during planning and those that reason about communication during execution.  \u000a Planning time approaches~\u005ccite{goldman2003optimizing,pynadath2002communicative,spaan2006decentralized} assume that all possible observations that agents might receive and all possible states they might encounter are known at planning time, and they produce a \u005cemph{joint policy} that all agents follow. In contrast, SATD models situations in which an individual agent learns new information that was not available or planned for at planning time, and it produces a communication policy for that \u005cemph{single agent}. \u000aExecution time approaches to communication~\u005ccite{xuan2001communication,oliehoek2007dec,emery2005game,roth2005reasoning,roth2006communicate,wu2009multi} do not plan when to communicate ahead of time, but rather decide at every time step whether to communicate or not. Execution-time approaches reduce computational complexity as agents only reason about communication given their actual observations. These approaches, however, are myopic, potentially leading to non-optimal behavior. More importantly, these execution time approaches cannot be applied to  the SATD problem because they assume that all possible observations are known at planning time and that a centralized planner generated policies for the agents. In contrast, the MDP-PRT agent is not myopic; it allows agents to plan to communicate at later stages of execution. It also allows for unanticipated information and assumes that agents individually generate plans for the constituent tasks for which they are responsible. Because there is no central planning, even if information was anticipated by one of the agents at planning time, it might not have be considered by all agents.\u000aPrior work on integrating BDI and DT models includes ``translations'' between these models~\u005ccite{simari2006relationship,schut2002partially} and formal ways of integrating them~\u005ccite{maheswaran2004adjustable,nair2003integrating}.  Work that combines BDI concepts with decision theoretic approaches to reason about communication includes Kwak et al.~\u005cshortcite{kwak2011robust} who define BDI inspired ``trigger points'' for communication in a DEC-POMDP framework, and Kamar et al.~\u005cshortcite{kamar2009incorporating} who developed PRTs to represent collaborative activities which we discuss in the approach section.  Interactive POMDPs~\u005ccite{gmytrasiewicz2005framework} also integrate beliefs about other agents in an MDP framework.\u000a In I-POMDPs, beliefs are over agent ``types'' which are distinguished by an agent's optimality criteria and other canonical POMDP components.  In contrast, in MDP-PRT the beliefs are over collaborating agents' plans and are compactly represented in PRTs. \u000aThis section proposes a solution to the 2-agent SATD problem $\u005clangle a_1, a_2, b_{SP}, V, o^*, \u005cvarphi_{comm}, C \u005crangle$, where an agent $a_1$ learns new information $o^*$ and needs to reason whether and when to communicate $o^*$ to $a_2$. \u000aTo solve the 2-agent SATD communication problem, we use an MDP in which the states explicitly represent $a_1$'s beliefs about $a_2$'s plans. \u000aThe choice of representation for the agent's beliefs $b_{SP}$ is key, as it affects the way $b_{SP}$ can be revised and therefore the computational efficiency of solving the MDP. Our approach uses a PRT to represent $b_{SP}$. Henceforth we refer to the integrated MDP-PRT representation as ``MDP-PRT''.\u000aThe PRT representation extends the SharedPlans~\u005ccite{grosz1996collaborative} formalization by introducing decision-theoretic notions  of uncertainty and utilities to the hierarchical plan representation for recipes. \u000aFormally,  a PRT for a complex action defines a probability distribution over the possible recipes (plans) for accomplishing that action. The recipes are represented in an AND/OR tree. AND nodes represent complex actions that need to be carried out. Probabilities over OR branches  represent the agent's belief about its partners' possible alternatives for carrying out these complex actions.\u000a  We chose PRTs because they are a compact representation and their decomposable structure allows reasoning about non-dependent activities separately.\u000a In the MDP-PRT $\u005clangle A, S, R, Tr, s_0 \u005crangle$, $A$ includes two actions {\u005cem inform} (communicating $o^*$) and $\u005cneg${\u005cem inform} (not communicating $o^*$). Each state in $S$ encompasses $a_1$'s beliefs about $a_2$'s plans (i.e., the PRT corresponding to $b_{SP}$).\u005cfootnote{$b_{SP}$ also includes $a_1$'s own plans but $a_1$ does not need to reason about them in the context of information sharing.} \u000a We denote a state by $b_{SP}$.\u000a The initial state ${b_{SP}}_{0}$ corresponds to $a_1$'s initial beliefs about the SharedPlan.\u000aThe reward function is a function of $V$ and $C$: the reward for a state $b_{SP}$ is the  value of the constituent tasks completed in the last time step minus the cost of communication if $a_1$ chose to inform $a_2$. \u000aThe transition function, $Tr({b_{SP}}',a,b_{SP})$, defines the probability of reaching state ${b_{SP}}'$, when taking action $a$ in state $b_{SP}$. $a_1$'s belief may change for two different reasons. First, if $a_1$ communicates $o^*$ to $a_2$, then $b_{SP}$ changes to $\u005cvarphi_{comm}(b_{SP},o^*)$. \u000aSecond, as $a_2$ executes actions in its constituent plans, $a_1$ may ``observe'' $a_2$'s actions or results of those actions and learn more about $a_2$'s plans. To reflect this reasoning, we define an additional function, $\u005cvarphi_{obs}(b_{SP})$. This function takes as input $b_{SP}$ and returns the set of next expected beliefs $\u005cmathbf{b_{SP}^{next}}$ and their probabilities $\u005cmathbf{Pr(b_{SP}^{next})}$. \u000a To illustrate, we consider the care coordination problem. The primary care provider (PCP) has beliefs about the neurologist's treatment options and the likelihood each might be chosen. When the PCP considers whether to share new information about the patient (e.g. that the child had a seizure), she reasons about the effect of this information on the neurologist's plans ($\u005cvarphi_{comm}$). Whether or not she decides to inform the neurologist, her beliefs also evolve with time as she learns about actions the neurologist has executed ($\u005cvarphi_{obs}$). For example, if she learns that the neurologist ordered certain lab tests, she might revise her beliefs if the lab tests are consistent with some plans but not with others.\u000aAlgorithm~\u005cref{alg:Al} gives the pseudo-code for $Tr({b_{SP}}',a,b_{SP})$. First, if $a_1$ chooses to inform, $b_{SP}$ is updated using $\u005cvarphi_{comm}$ (lines 2-3). If  ${b_{SP}}'$ is one of the possible next states according to $\u005cvarphi_{obs}$, its probability based on $\u005cvarphi_{obs}$ is returned (lines 4-6). If it's not included in the set of next possible beliefs the transition probability is 0 (line 7). \u000aWe note that although each state encompasses a probability distribution over plans, the state space of the MDP is not continuous because the set of considered possible beliefs is discrete and finite. \u000a$Pr({b_{SP}}') \u005cgets 0$\u005c;\u000a	$b_{SP} = \u005cvarphi_{comm}(b_{SP},o^*)$\u000a$ \u005cmathbf{\u005clangle b_{SP}^{next}, Pr(b_{SP}^{next})\u005crangle}$ = $\u005cvarphi_{obs}(b_{SP})$\u005c;\u000a	\u005cReturn {$Pr({b_{SP}}')$ according to $\u005cmathbf{Pr(b_{SP}^{next})}$}	\u000a An optimal single agent communication policy can be computed using any MDP solver, e.g. value iteration algorithm~\u005ccite{sondik1971optimal} used in our implementation. This policy computation is performed when a new observation $o^*$ is obtained by an agent $a_i$ at time $t$. During subsequent rounds, the agent uses the computed policy to decide whether to inform $a_2$ of $o^*$. \u000a The MDP-PRT can be viewed as a decision-theoretic representation that supports the monitoring and execution of a SharedPlan. While we focused on the question of sharing new information, a similar approach can be taken to reason about whether to help a teammate or ask teammates for information~\u005ccite{kamar2009incorporating}.  We also note that while in our empirical domain agents could observe each other's actions, the MDP-PRT can also be used if not all actions are observable by modifying $\u005cvarphi_{obs}$ to consider the possibility of not observing an action.\u000a   We discuss possible extensions of the MDP-PRT to larger groups of agents in the discussion section.  \u000aWe tested the MDP-PRT agent using a modified version of the Colored Trails (CT) game used by Kamar et al.~\u005cshortcite{kamar2009incorporating}.  An example configuration of the game is shown in Figure~\u005cref{fig:game}\u000aThe game is played by two agents, the \u005cemph{Partner} (PAR) and the \u005cemph{Observer} (OBS), on a $4\u005ctimes 4$ board. Each square in the board is of one of four colors. At the onset of the game, \u005cemph{PAR}  is located on a board square ($(0,0)$ in Figure~\u005cref{fig:game}). \u000aIt possesses chips of the four colors and uses them to move on the board: it can move to an adjacent square (vertically or horizontally) by giving up a chip of the same color as the destination square. Its task is to reach the goal square. \u000aEach square has some probability that a trap will appear on it based on its color. Traps are represented by the ``bomb'' icon. When an agent moves into a trap square, it cannot move further and the game ends. PAR has uncertain knowledge about the distribution of traps (i.e. the probability a trap will appear on each color), but does not know their exact locations.  OBS learns the exact locations of traps at the onset of the game.  It also knows the chips that  PAR possesses and  its knowledge regarding trap distributions. \u000aThe game is cooperative and the score (utility)  is calculated as follows: the agents receive 100 points if  PAR reaches the goal and 5 points for each remaining chip it holds at the end of the game. If PAR does not reach the goal, agents are penalized according to its distance from the goal (10 times Manhattan distance from the goal). \u000aEach turn in the game consists of two phases: In the first phase, the communication phase, OBS  can  choose to perform an \u005cemph{inform} action that will reveal trap locations to PAR. Informing, however, incurs  a cost that is deducted from the agents' final score. OBS therefore needs to reason about the utility of an inform action by weighing the benefits and costs entailed by it. In the second, movement, phase, PAR moves using its chips. The game ends when one of the following occurs: PAR reaches the goal or moves to a trap location, or after a maximal fixed number of turns passes.\u000a  \u005cvspace{-0.5cm}\u000aThis game setting encompasses some of the key characteristics of the healthcare domain and enables studying them more abstractly: (1)  PAR's alternative plans and the uncertain knowledge of  OBS about these plans correspond to caregivers' alternative treatment plans and their incomplete knowledge of others' treatment plans; (2) OBS seeing  PAR's location  corresponds to a caregiver observing other caregivers' actions by reading notes in the medical record; (3) learning about traps corresponds to caregivers detecting problems with a treatment plan after observing a change in patient status, and (4) communication is costly due to caregivers'  limited time and many responsibilities. \u000a We implemented OBS as an MDP-PRT agent that makes the decision of whether to share trap locations ($o^*$) with PAR. PAR's task of getting as close as possible to the goal represents its constituent task in the collaborative activity of OBS and PAR. At the onset of the game, when OBS learns the true locations of traps, value iteration is run to generate a communication policy. \u000a  The initial PRT, representing OBS's beliefs  about PAR's possible plans ($b_{SP}$), is derived from the board and from OBS's knowledge of PAR's knowledge of trap distributions. For example, if OBS knows PAR has no knowledge about trap locations,  it assigns equal probabilities to all shortest paths that PAR can take. \u000a$\u005cvarphi_{comm}$ revises  OBS's beliefs assuming it communicates the trap locations to PAR. It computes new shortest paths assuming that PAR will choose one of the shortest paths towards the goal that do not pass through any traps. For example, in the configuration shown in Figure~\u005cref{fig:game}, and assuming that PAR has no knowledge of trap locations, OBS's initial  $b_{SP}$ will assign equal probabilities to all shortest paths from PAR's location $(0,0)$ to its goal $(2,3)$. When considering the ``inform'' action in this state,\u000a  the modified belief according to $\u005cvarphi_{comm}$ will assign equal probabilities to all shortest paths that do not pass through $(2,0)$. \u000a   \u000a   Whether or not OBS chooses to inform PAR, the generation of the next possible states ${b_{SP}}'$ also takes into account PAR's next possible movements (which OBS will observe during execution). The states correspond to $a_1$'s modified beliefs according to $\u005cvarphi_{obs}$. That is, OBS knows it will observe PAR's movements at execution time and reasons about the possible changes in the probabilities of different paths. For each possible movement, $\u005cvarphi_{obs}$ generates a new state ${b_{SP}}'$ by eliminating plans that do not include that movement and re-normalizing the probabilities of the remaining paths.   \u000a   \u000a   During the game, OBS revises its belief (${b}_{SP}$) at each turn by eliminating paths that do not include the observed movements. It decides whether to communicate based on its computed policy.\u000a   \u000a   \u000a   \u000aTo evaluate the MDP-PRT agent we ran several types of experiments. \u000aThe first experiment compared the performance of the MDP-PRT agent with that of a PRT agent using the inform algorithm of Kamar et al.~\u005cshortcite{kamar2009incorporating}. The algorithm decides whether to inform at the onset of the game by comparing the expected utility of the PRT representing OBS's initial beliefs with the expected utility of a revised PRT describing the belief about the plans if trap locations are shared ($\u005cvarphi_{comm}$). If the increase in communication cost is greater than the communication cost, the agent communicates. This approach is myopic as it does not consider the possibility of waiting to learn more about PAR's plans and possibly communicating at a later stage.\u000a We used 6 different board configurations varying the trap distribution in different colors. We generated 6 board instances for each trap distribution and varied the communication cost between runs. We ran each combination of board instance and communication cost 10 times.  In all games, PAR  knew the trap distribution but not the actual trap locations; its moves were chosen randomly based on its expected utility (e.g. it chose between shortest paths weighing trap probability).  OBS  knew that PAR knew only the trap distribution. PAR randomly chose one of its shortest paths and moved accordingly. Once  OBS  communicated the trap locations, PAR randomly chose a shortest path that do not pass through any traps.\u000aTable~\u005cref{table:res} shows the average utility achieved by the agents in experiments with different communication costs. Results are averaged across all board instances and the 10 runs of each instance. The maximal utility that could be obtained in the game ranged between 100 and 125, depending on  PAR's chips and the goal location. Thus a communication cost of 10 is about 10\u005c% of the maximal possible utility.  As expected, the MDP-PRT agent outperforms the PRT agent across all configurations and communication costs.  All reported differences are statistically significant ($P<0.01$).\u000a   \u005cbegin{table}[h]\u000a   \u005ccentering\u000a   \u005csmall\u000a   \u005cresizebox{5cm}{!}{%\u000a   \u005cbegin{tabular}{|c|c|c|c|c|c|}\u000a    \u005chline\u000a  \u005ctextbf{Comm. Cost} & \u005ctextbf{5}  & \u005ctextbf{10} & \u005ctextbf{20} &  \u005ctextbf{30}\u005c\u005c \u000a   \u005chline\u000a   \u005chline\u000a  \u005ctextbf{PRT} & 67.67  & 65.17 & 60.17 & 55.17 \u005c\u005c \u000a   \u005chline\u000a  \u005ctextbf{MDP-PRT}   & \u005ctextbf{77} & \u005ctextbf{75.57} & \u005ctextbf{72.83} & \u005ctextbf{70.03 }\u005c\u005c \u000a   \u005chline   \u000a   \u005cend{tabular}}\u000a   \u005ccaption{The average utility for each agent and communication cost, averaged over all board instances. }\u000a   \u005clabel{table:res}\u000a     \u005cvspace{-0.2cm}\u000a   \u005cend{table}\u000a   \u000a   \u000a   The MDP-PRT agent saves unnecessary communication and thus outperforms the PRT agent. To illustrate this difference, we consider the board configuration shown in Figure~\u005cref{fig:game}. In the first round of the game,  PAR   \u000a   is 2 squares away from the trap. When the PRT agent decides whether to communicate, it considers the distribution over possible paths that  PAR might choose. One of these paths goes through the trap; so there is some probability that  PAR will reach the trap location. The OBS (PRT) agent will decide to communicate if the difference between the expected utility of the updated PRT after informing and the expected utility of the PRT representing  PAR's plans without informing is higher than the communication cost. In contrast, the MDP-PRT agent would also take into account the possibility of informing at a later stage: Since PAR is two squares away from the trap, the MDP-PRT agent will choose not to inform at this turn, since if PAR would move right in the next turn, then it is unlikely it chose a plan that passes through a trap location. \u000a   Figure~\u005cref{fig:util}(a)  shows the average utilities achieved by the agents in boards with probability of 0.15 for traps to appear on any board square; Figure~\u005cref{fig:util}(b) shows the average utilities in  boards with probability of 0.15 for traps to appear on any red or green square. (there are more traps on the board configurations in (a) than in (b)). \u000a   As can be seen in the figure, the MDP-PRT agent outperformed the baseline PRT agent in both. \u000a   The difference in utilities grows with communication cost. When communication cost increases, the MDP-PRT agent benefits more from avoiding unnecessary  communication. In the configuration shown in Figure~\u005cref{fig:util}(a) we also observe a higher decrease in utilities for the MDP-PRT agent. This is a result of the combination of boards that have more traps together with the high communication costs. \u000a  \u005cbegin{figure}[h]\u000a          \u005ccentering\u000a          \u005cbegin{subfigure}[b]{0.25\u005ctextwidth}\u000a                  \u005ccentering\u000a                  \u005cincludegraphics[width=4cm]{figures/fig2bCropped.pdf}\u000a                  \u005ccaption{All 0.15 configuration}\u000a                  \u005clabel{fig:all}\u000a          \u005cend{subfigure}%\u000a          ~ %add desired spacing between images, e. g. ~, \u005cquad, \u005cqquad etc.\u000a            %(or a blank line to force the subfigure onto a new line)\u000a          \u005cbegin{subfigure}[b]{0.25\u005ctextwidth}\u000a                  \u005ccentering\u000a                  \u005cincludegraphics[width=4cm]{figures/fig2aCropped.pdf}\u000a                  \u005ccaption{RG 0.15 configuration }\u000a                  \u005clabel{fig:r}\u000a          \u005cend{subfigure}\u000a          \u005ccaption{Average utilities obtained by the agents: (a) boards with probability of 0.15 for traps to appear on any color; (b) boards with probability 0.15 for traps to appear on any red or green squares.}\u000a          \u005clabel{fig:util}\u000a            \u005cvspace{-0.3cm}\u000a  \u005cend{figure}\u000a  Reasoning about the future, however, has a cost in terms of computation time. The MDP-PRT agent needs to generate and solve the MDP for the given game configuration, which requires more computation than the one-shot decision made by the PRT agent at the onset of the game. On average, decision time was about two times slower when using the MDP-PRT agent (mean = 29.82 msec), as compared to the PRT agent's (mean = 13.71 msec). \u000a  \u005csubsection{Comparison with Dec-POMDP}\u000aKey aspects of SATD are that agents cannot anticipate $a_1$ observing $o^*$ nor form a coordinated policy. This limits the possible utility achievable in SATD, as a coordinated policy that takes into account knowledge of possible observations during planning time can lead to better performance.\u000a  We illustrate this using the board instance shown in Figure~\u005cref{fig:game}. The MDP-PRT agent can delay its decision about communication to the second round, after observing the new location of PAR. If  PAR moves to (1,0), the agent might decide to communicate. \u000aA Dec-POMDP with prior knowledge of the possible observations can do better. Had the agents known in advance that traps might appear only on the two red squares and that OBS will learn the true trap locations, they could have agreed on a communication policy, allowing  PAR to learn something about the trap locations even when  OBS does not communicate. For example, they could have agreed that OBS will communicate if the trap is in location $(1,2)$ but not if it is on $(2,0)$. Then, if OBS does not communicate, PAR learns that there is a trap on $(2,0)$.  \u000aTo compute a policy for the setting in which agents \u005cemph{anticipate} that OBS will learn about trap locations, we solve a Dec-POMDP that has information about the trap distributions, and knows that  OBS will learn the true locations of traps.  A state in the Dec-POMDP for this game includes the true location of the traps, the location of  PAR, its chips and its knowledge of the trap distribution. For example, if traps can appear only on red squares, a board configuration with two red squares such as the board shown in Figure~\u005cref{fig:game} will induce four possible initial worlds states: no traps on the board; 2 traps on the board, one on (2,0) and one on (1,2); a trap on (1,2) but not on (2,0); a trap on (2,0) but not on (1,0) (the real world state shown in the figure). These four initial states also describe the location of  PAR ($(0,0)$ in the example), its chips and its knowledge of the trap distribution. At the onset of the game,  OBS observes the true state of the world (i.e. the true trap locations). \u000a Solving the Dec-POMDP results in an optimal joint policy that achieves the maximal utility. For  PAR, the policy specifies movement actions. For  OBS, the policy specifies whether or not to inform  PAR of the trap locations at each communication stage.  Note that the Dec-POMDP policy so it can  always perform at least as well as the MDP-PRT agent, as it has more knowledge about the world.\u000a   \u000aAs a result of the computational complexity of the Dec-POMDP, we restricted the experiments to boards that had at most two possible trap locations.\u000aWe varied the probability that a possible trap location  actually includes a trap, with values  $P_t = 0.2, 0.5, 0.8$, and generated 6 board configurations for each of these values. $P_t$  determines the probability distribution over world states. For example, in the board shown in Figure~\u005cref{fig:game}, the two red squares are possible trap locations. If $P_t = 0.2$, then the probability of the world state shown in Figure~\u005cref{fig:game}  is $P_t \u005ccdot (1-P_t)=0.2 \u005ccdot 0.8=0.16$ because (2,0) has a trap while (1,2) does not.\u000a   A Dec-POMDP policy maximizes the expected utility over all possible world states while MDP-PRT is run on a particular world state (e.g. a particular board with traps already determined). Thus, for each board configuration and trap distribution, we ran MDP-PRT on boards representing all possible world states and compute a weighted average of their utility based on the distribution over world states. For example, the board instance in Figure~\u005cref{fig:game} is one of four instances for a board configuration with possible traps on squares (2,0) and (1,2). The other three instances include a board with no traps, a board with a trap in (1,2) but not in (2,0) and a board with traps on both locations.   We ran the MDP-PRT agent on each of these boards to compare it with the Dec-POMDP policy.\u000a   \u000a   The first two rows in Table~\u005cref{tab:decCompare} show the utility obtained by the MDP-PRT agent and the expected utility of a joint policy generated by a Dec-POMDP.\u000a   These utilities are averaged over all 6 board configurations and trap probabilities. \u000a   \u000a    \u000a    \u005cbegin{table}[h]\u000a    \u005cvspace{-0.2cm}\u000a    \u005ccentering\u000a    \u005csmall\u000a     \u005cresizebox{6.5cm}{!}{%\u000a    \u005cbegin{tabular}{c|c|c|c|c}\u000a    {} &5         & 10          & 25          & 50                      \u005c\u005c\u000a       \u005chline\u000a          \u005chline\u000a    Dec-POMDP & \u005cbf{101.8} & \u005cbf{101.46} & \u005cbf{100.2} & \u005cbf{98.13} \u005c\u005c\u000a       \u005chline\u000a    MDP-PRT (accurate)  & 100.52 & 99.08 & 94.38      & 87.5\u005c\u005c\u000a       \u005chline\u000a        MDP-PRT (inaccurate)  & 99.75 & 97.46 & 92.13      & 83.67\u005c\u005c\u000a           \u005chline\u000a    \u005cend{tabular}}\u000a    \u005ccaption{The performance of MDP-PRT (with accurate and inaccurate beliefs) and an optimal Dec-POMDP approach.}\u000a     \u005clabel{tab:decCompare}\u000a       \u005cvspace{-0.3cm}\u000a    \u005cend{table}\u000a   \u000a  As expected, the Dec-POMDP always performs better as a result of its additional information at planning time. The difference is statistically significant ($P<0.01$).  \u000a  As can be seen in the table, increasing communication cost leads to larger differences between the Dec-POMDP and MDP-PRT utilities, a result of the additional communication required by the MDP-PRT agent. When communication cost is up to 10\u005c% of the utility of reaching the goal (100 points), MDP-PRT compares well with the Dec-POMDP policy (less than 3\u005c% difference). When communication cost rises to 25\u005c% or 50\u005c% of goal utility, the difference grows, but the average utility achieved by MDP-PRT is still within 15\u005c% of the optimal utility. Increasing the trap probability decreases the utility obtained by both agents; it also increases the gap between MDP-PRT and the  Dec-POMDP because world states that include more traps and require more communication are more likely.\u000a \u005csubsection{Inaccurate Beliefs About Others' Plans}\u000aThe MDP-PRT agent requires knowledge of the probability distributions over the plans of other agents and a way to update this distribution given new information. In practice, it is possible (and likely) that agents will estimate these probabilities inaccurately. To examine the possible effects of such inaccuracies on agents' performance, we ran additional experiments. In these experiments  OBS  knew a probability distribution over PAR's chips, but did not know the actual chips  PAR had. Specifically, it assumed a uniform distribution between 1 to 4 chips of each color.\u000a Consequently, the initial PRT and updates to the PRT could be inaccurate in their assignment of probabilities to possible plans. \u000aThe last row of Table~\u005cref{tab:decCompare} shows the performance of the MDP-PRT agent using this inaccurate PRT estimation. As expected, inaccurate beliefs lead to a decrease in utility ($P<0.05$). In these settings, however, the average decrease in utility was small (less than 5\u005c%). Figure~\u005cref{fig:inacc} shows the performance of  three different agents on a particular board configuration that included only one possible trap location with probability 0.5 of a trap  on that location. In this configuration, the Dec-POMDP never communicates and thus always obtains the maximal possible utility. Compared to the MDP-PRT with an accurate PRT representing PAR's possible plans, the inaccurate model communicated more often, resulting in lower utility. Its inaccurate probability estimation assigned higher probability to paths that pass through traps than their actual probability. In general, wrong estimation of the PRT can lead to communicating too much, or communicating too little. \u000a	\u005cbegin{figure}\u000a	\u005ccentering\u000a	\u005cincludegraphics[width=4.3cm]{figures/fig3cropped.pdf}\u000a	%\u005ccaption{The utility achieved by the agents in board configurations with one possible trap location.}\u000a	\u005ccaption{Utility for board configurations with one possible trap.}\u000a	\u005clabel{fig:inacc}\u000a	    \u005cvspace{-0.5cm}\u000a	\u005cend{figure}\u000aThis paper defines the Single Agent in a Team Decision Problem: an agent obtains new, unanticipated, information during a collaborative activity and needs to reason about whether and when to share this information with its teammates. This work is a first step in our effort to develop computer agents to support the care team of children with complex conditions. In this healthcare domain -- a case of dynamic mixed networks comprising people and computational agents -- teams are unlikely to have a fully coordinated joint plan or be able to anticipate all possible events. \u000aWe thus encode the \u000apartial uncertain knowledge about the team's plan in a PRT instead of explicitly modeling all possible world observations (as is typically required in DT planning approaches). The PRT compactly represents many different possible plans and thus (indirectly) many possible observations as group activities evolve. For example, if an agent learns that its teammate chose a specific recipe, it can eliminate other possibilities at that OR node. Generally, either low-level or complex actions might be ``observed'' and the agent's beliefs can be revised by modifying the PRT accordingly. PRTs do not however support reasoning about {\u005cem when} to share new observations which  DT approaches do. We thus propose an integrated BDI/DT approach. In this approach, an MDP reasons about communication using the PRT representation of other agents' possible plans (in the context provided by the SharedPlan).\u000aTwo extensions of this work are required to use the MDP-PRT approach for agents operating in  mixed-networks, each a general multi-agent system challenge. To solve SATD for larger groups of agents requires deciding with whom to communicate, which itself requires taking into account interactions among agents' plans. We intend to use the decomposability of a team's plan to enable focusing on relevant agents and relevant aspects of their plans. The second challenge is developing methods for agents to acquire knowledge about other agents' possible plans. We are exploring several possibilities in the healthcare domain. Agents can infer an initial belief about their teammates' plans based on the partial high-level plan defined by the team. For example,  there are often guidelines for treating different conditions which can be used to reason about alternative plans of care providers. \u000aAlso, agents may be able to observe some of their teammates' actions and refine their beliefs using plan recognition techniques. For teams collaborating over a long time horizon, learning could be used to estimate others' plans and how they change with new information.\u000a
p11
sS'paragraphs'
p12
(lp13
(iversion
Paragraph
p14
(dp15
g10
VThis paper defines a new multi-agent decision problem, the ``Single Agent in a Team Decision" ({\u005cem SATD}) problem, which may be described informally as follows: An individual collaborating in a multi-agent team obtains new information, unanticipated at planning time.  This (single) agent has incomplete knowledge of others' plans.  It must decide whether to communicate this new information to its teammates, and if so, to whom, and at what time.  SATD differs from previously studied multi-agent communications problems in that it does not assume complete knowledge of other agents' plans or policies nor that all observations are knowable in advance.  It assumes instead that agents have some knowledge of each other's intentions and plans which can be used to reason about information sharing decisions.  SATD arises from the ways in which effective teamwork decomposes complex activities into constituent tasks and delegates responsibility for those tasks to team members with appropriate expertise and capabilities.  Human teammates typically make only general plans and allocate tasks at a high level of abstraction. They do not necessarily know each other's plans  nor consider together all contingencies of all possible plans for doing those tasks. Their cognitive load is lowered significantly as a result. To make appropriate decisions about sharing information, they must reason  with only uncertain knowledge of their teammates' plans. Agents participating in mixed networks comprising  humans and computer agents or supporting people in team settings also must be capable of such reasoning.  The SATD problem can also arise in purely computer-agent teamwork settings. For example, in ad hoc teamwork~\u005ccite{stone2010ad}, an agent joining an existing team only after planning time  lacks significant information about other agents' plans, but might still need to reason about which observations to share with teammates. We are investigating SATD in the context of developing computer agents to support care teams for children with complex conditions.  Agents able to support health-care teams by identifying what information to share, with whom and when have the potential to substantially improve health outcomes. Care teams for children with complex conditions typically involve many providers -- a primary care provider, specialists, therapists, and non-medical care givers. The care team defines a high-level care plan that describes the main care goals, but there is no centralized planning mechanism that generates a complete plan for the team or that can ensure coordination. Caregivers are unaware of their collaborators' complete plans, yet their individual plans often interact. Communicating relevant information among team members is crucial for care to be coordinated and effective, but doing so is costly and often insufficient in practice.
p16
sS'changed'
p17
I00
sS'nextindex'
p18
NsS'lastindex'
p19
Nsba(iversion
Paragraph
p20
(dp21
g10
VTo address SATD, the paper proposes a novel, integrated Belief-Desire-Intention (BDI) and decision-theoretic (DT) representation thatbuilds on the strengths of each approach.  In particular, our approach integrates the Probabilistic Recipe Trees (PRT) representation of an agent's beliefs about another agent's plans~\u005ccite{kamar2009incorporating} with a Markov Decision Process (MDP) to support a collaborating group in their execution of a plan. We call this integrated representation  MDP-PRT. We evaluated an agent using MDP-PRT to solve the SATD problem in an abstract setting. Results show that it outperforms the inform algorithm proposed by Kamar et al.~\u005cshortcite{kamar2009incorporating}. In addition, we compared the agent's performance with that of Dec-POMDP policy that was informed about all possible observations.
p22
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p23
(dp24
g10
VThe MDP-PRT agent obtains results close to those obtained by this Dec-POMDP policy despite lacking a  coordinated policy that considers all possible observations.
p25
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p26
(dp27
g10
VThe paper makes three contributions. First, it formally defines  the SATD communication problem and contrasts it with previously studied communication problems in multi-agent settings. Second, it proposes a new representation (MDP-PRT)  that enables agents to reason about and solve the SATD communication problem. Third, it demonstrates the usefulness of this representation and analyzes the effect of agents' uncertainties and of communication cost on the performance of a team.
p28
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p29
(dp30
g10
VIn this section, we formally define the SATD problem. SATD arises in the context of a group activity of a team of agents.  We assume the group's plan meets the SharedPlans specification for collaborative action~\u005ccite{grosz1996collaborative}.  Two particular properties of SharedPlans are important:  plans, which decompose into constituent tasks, may be only partially specified and agents do not know the details of constituent tasks for which they are not responsible.
p31
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p32
(dp33
g10
V An instance of the SATD problem  is represented by a tuple $\u005cbf{\u005clangle a_i, A_{-i}, b_{SP}, V, o^*, \u005cvarphi_{comm}, C \u005crangle}$:SATD is the problem of $a_i$ determining whether to communicate $o^*$ to agents in  $A_{-i}$ and if so, at what time. It has two components: determining which agents are candidates for receiving the information and deciding whether, and when, to send information to (some or all) candidates.  This paper focuses on the constituent problem of deciding whether and when to send information. Theories of teamwork and collaboration~\u005ccite{grosz1996collaborative,cohen1990intention,sonenberg1992planned} emphasize the key role of communication in teamwork. BDI approaches to multi-agent planning, e.g. STEAM~\u005ccite{tambe1997agent}, often base their communication mechanisms on these theories. BDI approaches typically do not reason about uncertainty and utilities~\u005ccite{pynadath2002communicative}. Prior work on decision theoretic approaches to multi-agent communication can generally be classified into two types: approaches that reason about communication during planning and those that reason about communication during execution.   Planning time approaches~\u005ccite{goldman2003optimizing,pynadath2002communicative,spaan2006decentralized} assume that all possible observations that agents might receive and all possible states they might encounter are known at planning time, and they produce a \u005cemph{joint policy} that all agents follow. In contrast, SATD models situations in which an individual agent learns new information that was not available or planned for at planning time, and it produces a communication policy for that \u005cemph{single agent}. Execution time approaches to communication~\u005ccite{xuan2001communication,oliehoek2007dec,emery2005game,roth2005reasoning,roth2006communicate,wu2009multi} do not plan when to communicate ahead of time, but rather decide at every time step whether to communicate or not. Execution-time approaches reduce computational complexity as agents only reason about communication given their actual observations. These approaches, however, are myopic, potentially leading to non-optimal behavior. More importantly, these execution time approaches cannot be applied to  the SATD problem because they assume that all possible observations are known at planning time and that a centralized planner generated policies for the agents. In contrast, the MDP-PRT agent is not myopic; it allows agents to plan to communicate at later stages of execution. It also allows for unanticipated information and assumes that agents individually generate plans for the constituent tasks for which they are responsible. Because there is no central planning, even if information was anticipated by one of the agents at planning time, it might not have be considered by all agents.
p34
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p35
(dp36
g10
VPrior work on integrating BDI and DT models includes ``translations'' between these models~\u005ccite{simari2006relationship,schut2002partially} and formal ways of integrating them~\u005ccite{maheswaran2004adjustable,nair2003integrating}.  Work that combines BDI concepts with decision theoretic approaches to reason about communication includes Kwak et al.~\u005cshortcite{kwak2011robust} who define BDI inspired ``trigger points'' for communication in a DEC-POMDP framework, and Kamar et al.~\u005cshortcite{kamar2009incorporating} who developed PRTs to represent collaborative activities which we discuss in the approach section.  Interactive POMDPs~\u005ccite{gmytrasiewicz2005framework} also integrate beliefs about other agents in an MDP framework.
p37
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p38
(dp39
g10
V In I-POMDPs, beliefs are over agent ``types'' which are distinguished by an agent's optimality criteria and other canonical POMDP components.  In contrast, in MDP-PRT the beliefs are over collaborating agents' plans and are compactly represented in PRTs. This section proposes a solution to the 2-agent SATD problem $\u005clangle a_1, a_2, b_{SP}, V, o^*, \u005cvarphi_{comm}, C \u005crangle$, where an agent $a_1$ learns new information $o^*$ and needs to reason whether and when to communicate $o^*$ to $a_2$. To solve the 2-agent SATD communication problem, we use an MDP in which the states explicitly represent $a_1$'s beliefs about $a_2$'s plans. The choice of representation for the agent's beliefs $b_{SP}$ is key, as it affects the way $b_{SP}$ can be revised and therefore the computational efficiency of solving the MDP. Our approach uses a PRT to represent $b_{SP}$. Henceforth we refer to the integrated MDP-PRT representation as ``MDP-PRT''.
p40
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p41
(dp42
g10
VThe PRT representation extends the SharedPlans~\u005ccite{grosz1996collaborative} formalization by introducing decision-theoretic notions  of uncertainty and utilities to the hierarchical plan representation for recipes. Formally,  a PRT for a complex action defines a probability distribution over the possible recipes (plans) for accomplishing that action. The recipes are represented in an AND/OR tree. AND nodes represent complex actions that need to be carried out. Probabilities over OR branches  represent the agent's belief about its partners' possible alternatives for carrying out these complex actions.
p43
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p44
(dp45
g10
V  We chose PRTs because they are a compact representation and their decomposable structure allows reasoning about non-dependent activities separately.
p46
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p47
(dp48
g10
V In the MDP-PRT $\u005clangle A, S, R, Tr, s_0 \u005crangle$, $A$ includes two actions {\u005cem inform} (communicating $o^*$) and $\u005cneg${\u005cem inform} (not communicating $o^*$). Each state in $S$ encompasses $a_1$'s beliefs about $a_2$'s plans (i.e., the PRT corresponding to $b_{SP}$).\u005cfootnote{$b_{SP}$ also includes $a_1$'s own plans but $a_1$ does not need to reason about them in the context of information sharing.}  We denote a state by $b_{SP}$.
p49
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p50
(dp51
g10
V The initial state ${b_{SP}}_{0}$ corresponds to $a_1$'s initial beliefs about the SharedPlan.
p52
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p53
(dp54
g10
VThe reward function is a function of $V$ and $C$: the reward for a state $b_{SP}$ is the  value of the constituent tasks completed in the last time step minus the cost of communication if $a_1$ chose to inform $a_2$. The transition function, $Tr({b_{SP}}',a,b_{SP})$, defines the probability of reaching state ${b_{SP}}'$, when taking action $a$ in state $b_{SP}$. $a_1$'s belief may change for two different reasons. First, if $a_1$ communicates $o^*$ to $a_2$, then $b_{SP}$ changes to $\u005cvarphi_{comm}(b_{SP},o^*)$. Second, as $a_2$ executes actions in its constituent plans, $a_1$ may ``observe'' $a_2$'s actions or results of those actions and learn more about $a_2$'s plans. To reflect this reasoning, we define an additional function, $\u005cvarphi_{obs}(b_{SP})$. This function takes as input $b_{SP}$ and returns the set of next expected beliefs $\u005cmathbf{b_{SP}^{next}}$ and their probabilities $\u005cmathbf{Pr(b_{SP}^{next})}$.  To illustrate, we consider the care coordination problem. The primary care provider (PCP) has beliefs about the neurologist's treatment options and the likelihood each might be chosen. When the PCP considers whether to share new information about the patient (e.g. that the child had a seizure), she reasons about the effect of this information on the neurologist's plans ($\u005cvarphi_{comm}$). Whether or not she decides to inform the neurologist, her beliefs also evolve with time as she learns about actions the neurologist has executed ($\u005cvarphi_{obs}$). For example, if she learns that the neurologist ordered certain lab tests, she might revise her beliefs if the lab tests are consistent with some plans but not with others.
p55
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p56
(dp57
g10
VAlgorithm~\u005cref{alg:Al} gives the pseudo-code for $Tr({b_{SP}}',a,b_{SP})$. First, if $a_1$ chooses to inform, $b_{SP}$ is updated using $\u005cvarphi_{comm}$ (lines 2-3). If  ${b_{SP}}'$ is one of the possible next states according to $\u005cvarphi_{obs}$, its probability based on $\u005cvarphi_{obs}$ is returned (lines 4-6). If it's not included in the set of next possible beliefs the transition probability is 0 (line 7). We note that although each state encompasses a probability distribution over plans, the state space of the MDP is not continuous because the set of considered possible beliefs is discrete and finite. $Pr({b_{SP}}') \u005cgets 0$\u005c;	$b_{SP} = \u005cvarphi_{comm}(b_{SP},o^*)$$ \u005cmathbf{\u005clangle b_{SP}^{next}, Pr(b_{SP}^{next})\u005crangle}$ = $\u005cvarphi_{obs}(b_{SP})$\u005c;	\u005cReturn {$Pr({b_{SP}}')$ according to $\u005cmathbf{Pr(b_{SP}^{next})}$}	 An optimal single agent communication policy can be computed using any MDP solver, e.g. value iteration algorithm~\u005ccite{sondik1971optimal} used in our implementation. This policy computation is performed when a new observation $o^*$ is obtained by an agent $a_i$ at time $t$. During subsequent rounds, the agent uses the computed policy to decide whether to inform $a_2$ of $o^*$.  The MDP-PRT can be viewed as a decision-theoretic representation that supports the monitoring and execution of a SharedPlan. While we focused on the question of sharing new information, a similar approach can be taken to reason about whether to help a teammate or ask teammates for information~\u005ccite{kamar2009incorporating}.  We also note that while in our empirical domain agents could observe each other's actions, the MDP-PRT can also be used if not all actions are observable by modifying $\u005cvarphi_{obs}$ to consider the possibility of not observing an action.
p58
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p59
(dp60
g10
V   We discuss possible extensions of the MDP-PRT to larger groups of agents in the discussion section.  We tested the MDP-PRT agent using a modified version of the Colored Trails (CT) game used by Kamar et al.~\u005cshortcite{kamar2009incorporating}.  An example configuration of the game is shown in Figure~\u005cref{fig:game}The game is played by two agents, the \u005cemph{Partner} (PAR) and the \u005cemph{Observer} (OBS), on a $4\u005ctimes 4$ board. Each square in the board is of one of four colors. At the onset of the game, \u005cemph{PAR}  is located on a board square ($(0,0)$ in Figure~\u005cref{fig:game}). It possesses chips of the four colors and uses them to move on the board: it can move to an adjacent square (vertically or horizontally) by giving up a chip of the same color as the destination square. Its task is to reach the goal square. Each square has some probability that a trap will appear on it based on its color. Traps are represented by the ``bomb'' icon. When an agent moves into a trap square, it cannot move further and the game ends. PAR has uncertain knowledge about the distribution of traps (i.e. the probability a trap will appear on each color), but does not know their exact locations.  OBS learns the exact locations of traps at the onset of the game.  It also knows the chips that  PAR possesses and  its knowledge regarding trap distributions. The game is cooperative and the score (utility)  is calculated as follows: the agents receive 100 points if  PAR reaches the goal and 5 points for each remaining chip it holds at the end of the game. If PAR does not reach the goal, agents are penalized according to its distance from the goal (10 times Manhattan distance from the goal). Each turn in the game consists of two phases: In the first phase, the communication phase, OBS  can  choose to perform an \u005cemph{inform} action that will reveal trap locations to PAR. Informing, however, incurs  a cost that is deducted from the agents' final score. OBS therefore needs to reason about the utility of an inform action by weighing the benefits and costs entailed by it. In the second, movement, phase, PAR moves using its chips. The game ends when one of the following occurs: PAR reaches the goal or moves to a trap location, or after a maximal fixed number of turns passes.
p61
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p62
(dp63
g10
V  \u005cvspace{-0.5cm}This game setting encompasses some of the key characteristics of the healthcare domain and enables studying them more abstractly: (1)  PAR's alternative plans and the uncertain knowledge of  OBS about these plans correspond to caregivers' alternative treatment plans and their incomplete knowledge of others' treatment plans; (2) OBS seeing  PAR's location  corresponds to a caregiver observing other caregivers' actions by reading notes in the medical record; (3) learning about traps corresponds to caregivers detecting problems with a treatment plan after observing a change in patient status, and (4) communication is costly due to caregivers'  limited time and many responsibilities.  We implemented OBS as an MDP-PRT agent that makes the decision of whether to share trap locations ($o^*$) with PAR. PAR's task of getting as close as possible to the goal represents its constituent task in the collaborative activity of OBS and PAR. At the onset of the game, when OBS learns the true locations of traps, value iteration is run to generate a communication policy.   The initial PRT, representing OBS's beliefs  about PAR's possible plans ($b_{SP}$), is derived from the board and from OBS's knowledge of PAR's knowledge of trap distributions. For example, if OBS knows PAR has no knowledge about trap locations,  it assigns equal probabilities to all shortest paths that PAR can take. $\u005cvarphi_{comm}$ revises  OBS's beliefs assuming it communicates the trap locations to PAR. It computes new shortest paths assuming that PAR will choose one of the shortest paths towards the goal that do not pass through any traps. For example, in the configuration shown in Figure~\u005cref{fig:game}, and assuming that PAR has no knowledge of trap locations, OBS's initial  $b_{SP}$ will assign equal probabilities to all shortest paths from PAR's location $(0,0)$ to its goal $(2,3)$. When considering the ``inform'' action in this state,  the modified belief according to $\u005cvarphi_{comm}$ will assign equal probabilities to all shortest paths that do not pass through $(2,0)$.       Whether or not OBS chooses to inform PAR, the generation of the next possible states ${b_{SP}}'$ also takes into account PAR's next possible movements (which OBS will observe during execution). The states correspond to $a_1$'s modified beliefs according to $\u005cvarphi_{obs}$. That is, OBS knows it will observe PAR's movements at execution time and reasons about the possible changes in the probabilities of different paths. For each possible movement, $\u005cvarphi_{obs}$ generates a new state ${b_{SP}}'$ by eliminating plans that do not include that movement and re-normalizing the probabilities of the remaining paths.         During the game, OBS revises its belief (${b}_{SP}$) at each turn by eliminating paths that do not include the observed movements. It decides whether to communicate based on its computed policy.
p64
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p65
(dp66
g10
V         To evaluate the MDP-PRT agent we ran several types of experiments. The first experiment compared the performance of the MDP-PRT agent with that of a PRT agent using the inform algorithm of Kamar et al.~\u005cshortcite{kamar2009incorporating}. The algorithm decides whether to inform at the onset of the game by comparing the expected utility of the PRT representing OBS's initial beliefs with the expected utility of a revised PRT describing the belief about the plans if trap locations are shared ($\u005cvarphi_{comm}$). If the increase in communication cost is greater than the communication cost, the agent communicates. This approach is myopic as it does not consider the possibility of waiting to learn more about PAR's plans and possibly communicating at a later stage.
p67
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p68
(dp69
g10
V We used 6 different board configurations varying the trap distribution in different colors. We generated 6 board instances for each trap distribution and varied the communication cost between runs. We ran each combination of board instance and communication cost 10 times.  In all games, PAR  knew the trap distribution but not the actual trap locations; its moves were chosen randomly based on its expected utility (e.g. it chose between shortest paths weighing trap probability).  OBS  knew that PAR knew only the trap distribution. PAR randomly chose one of its shortest paths and moved accordingly. Once  OBS  communicated the trap locations, PAR randomly chose a shortest path that do not pass through any traps.
p70
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p71
(dp72
g10
VTable~\u005cref{table:res} shows the average utility achieved by the agents in experiments with different communication costs. Results are averaged across all board instances and the 10 runs of each instance. The maximal utility that could be obtained in the game ranged between 100 and 125, depending on  PAR's chips and the goal location. Thus a communication cost of 10 is about 10\u005c% of the maximal possible utility.  As expected, the MDP-PRT agent outperforms the PRT agent across all configurations and communication costs.  All reported differences are statistically significant ($P<0.01$).
p73
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p74
(dp75
g10
V   \u005cbegin{table}[h]   \u005ccentering   \u005csmall   \u005cresizebox{5cm}{!}{%   \u005cbegin{tabular}{|c|c|c|c|c|c|}    \u005chline  \u005ctextbf{Comm. Cost} & \u005ctextbf{5}  & \u005ctextbf{10} & \u005ctextbf{20} &  \u005ctextbf{30}\u005c\u005c    \u005chline   \u005chline  \u005ctextbf{PRT} & 67.67  & 65.17 & 60.17 & 55.17 \u005c\u005c    \u005chline  \u005ctextbf{MDP-PRT}   & \u005ctextbf{77} & \u005ctextbf{75.57} & \u005ctextbf{72.83} & \u005ctextbf{70.03 }\u005c\u005c    \u005chline      \u005cend{tabular}}   \u005ccaption{The average utility for each agent and communication cost, averaged over all board instances. }   \u005clabel{table:res}     \u005cvspace{-0.2cm}   \u005cend{table}         The MDP-PRT agent saves unnecessary communication and thus outperforms the PRT agent. To illustrate this difference, we consider the board configuration shown in Figure~\u005cref{fig:game}. In the first round of the game,  PAR      is 2 squares away from the trap. When the PRT agent decides whether to communicate, it considers the distribution over possible paths that  PAR might choose. One of these paths goes through the trap; so there is some probability that  PAR will reach the trap location. The OBS (PRT) agent will decide to communicate if the difference between the expected utility of the updated PRT after informing and the expected utility of the PRT representing  PAR's plans without informing is higher than the communication cost. In contrast, the MDP-PRT agent would also take into account the possibility of informing at a later stage: Since PAR is two squares away from the trap, the MDP-PRT agent will choose not to inform at this turn, since if PAR would move right in the next turn, then it is unlikely it chose a plan that passes through a trap location.    Figure~\u005cref{fig:util}(a)  shows the average utilities achieved by the agents in boards with probability of 0.15 for traps to appear on any board square; Figure~\u005cref{fig:util}(b) shows the average utilities in  boards with probability of 0.15 for traps to appear on any red or green square. (there are more traps on the board configurations in (a) than in (b)).    As can be seen in the figure, the MDP-PRT agent outperformed the baseline PRT agent in both.    The difference in utilities grows with communication cost. When communication cost increases, the MDP-PRT agent benefits more from avoiding unnecessary  communication. In the configuration shown in Figure~\u005cref{fig:util}(a) we also observe a higher decrease in utilities for the MDP-PRT agent. This is a result of the combination of boards that have more traps together with the high communication costs.   \u005cbegin{figure}[h]          \u005ccentering          \u005cbegin{subfigure}[b]{0.25\u005ctextwidth}                  \u005ccentering                  \u005cincludegraphics[width=4cm]{figures/fig2bCropped.pdf}                  \u005ccaption{All 0.15 configuration}                  \u005clabel{fig:all}          \u005cend{subfigure}%          ~ %add desired spacing between images, e. g. ~, \u005cquad, \u005cqquad etc.
p76
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p77
(dp78
g10
V            %(or a blank line to force the subfigure onto a new line)          \u005cbegin{subfigure}[b]{0.25\u005ctextwidth}                  \u005ccentering                  \u005cincludegraphics[width=4cm]{figures/fig2aCropped.pdf}                  \u005ccaption{RG 0.15 configuration }                  \u005clabel{fig:r}          \u005cend{subfigure}          \u005ccaption{Average utilities obtained by the agents: (a) boards with probability of 0.15 for traps to appear on any color; (b) boards with probability 0.15 for traps to appear on any red or green squares.}          \u005clabel{fig:util}            \u005cvspace{-0.3cm}  \u005cend{figure}  Reasoning about the future, however, has a cost in terms of computation time. The MDP-PRT agent needs to generate and solve the MDP for the given game configuration, which requires more computation than the one-shot decision made by the PRT agent at the onset of the game. On average, decision time was about two times slower when using the MDP-PRT agent (mean = 29.82 msec), as compared to the PRT agent's (mean = 13.71 msec).   \u005csubsection{Comparison with Dec-POMDP}Key aspects of SATD are that agents cannot anticipate $a_1$ observing $o^*$ nor form a coordinated policy. This limits the possible utility achievable in SATD, as a coordinated policy that takes into account knowledge of possible observations during planning time can lead to better performance.
p79
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p80
(dp81
g10
V  We illustrate this using the board instance shown in Figure~\u005cref{fig:game}. The MDP-PRT agent can delay its decision about communication to the second round, after observing the new location of PAR. If  PAR moves to (1,0), the agent might decide to communicate. A Dec-POMDP with prior knowledge of the possible observations can do better. Had the agents known in advance that traps might appear only on the two red squares and that OBS will learn the true trap locations, they could have agreed on a communication policy, allowing  PAR to learn something about the trap locations even when  OBS does not communicate. For example, they could have agreed that OBS will communicate if the trap is in location $(1,2)$ but not if it is on $(2,0)$. Then, if OBS does not communicate, PAR learns that there is a trap on $(2,0)$.  To compute a policy for the setting in which agents \u005cemph{anticipate} that OBS will learn about trap locations, we solve a Dec-POMDP that has information about the trap distributions, and knows that  OBS will learn the true locations of traps.  A state in the Dec-POMDP for this game includes the true location of the traps, the location of  PAR, its chips and its knowledge of the trap distribution. For example, if traps can appear only on red squares, a board configuration with two red squares such as the board shown in Figure~\u005cref{fig:game} will induce four possible initial worlds states: no traps on the board; 2 traps on the board, one on (2,0) and one on (1,2); a trap on (1,2) but not on (2,0); a trap on (2,0) but not on (1,0) (the real world state shown in the figure). These four initial states also describe the location of  PAR ($(0,0)$ in the example), its chips and its knowledge of the trap distribution. At the onset of the game,  OBS observes the true state of the world (i.e. the true trap locations).  Solving the Dec-POMDP results in an optimal joint policy that achieves the maximal utility. For  PAR, the policy specifies movement actions. For  OBS, the policy specifies whether or not to inform  PAR of the trap locations at each communication stage.  Note that the Dec-POMDP policy so it can  always perform at least as well as the MDP-PRT agent, as it has more knowledge about the world.
p82
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p83
(dp84
g10
V   As a result of the computational complexity of the Dec-POMDP, we restricted the experiments to boards that had at most two possible trap locations.
p85
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p86
(dp87
g10
VWe varied the probability that a possible trap location  actually includes a trap, with values  $P_t = 0.2, 0.5, 0.8$, and generated 6 board configurations for each of these values. $P_t$  determines the probability distribution over world states. For example, in the board shown in Figure~\u005cref{fig:game}, the two red squares are possible trap locations. If $P_t = 0.2$, then the probability of the world state shown in Figure~\u005cref{fig:game}  is $P_t \u005ccdot (1-P_t)=0.2 \u005ccdot 0.8=0.16$ because (2,0) has a trap while (1,2) does not.
p88
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p89
(dp90
g10
V   A Dec-POMDP policy maximizes the expected utility over all possible world states while MDP-PRT is run on a particular world state (e.g. a particular board with traps already determined). Thus, for each board configuration and trap distribution, we ran MDP-PRT on boards representing all possible world states and compute a weighted average of their utility based on the distribution over world states. For example, the board instance in Figure~\u005cref{fig:game} is one of four instances for a board configuration with possible traps on squares (2,0) and (1,2). The other three instances include a board with no traps, a board with a trap in (1,2) but not in (2,0) and a board with traps on both locations.   We ran the MDP-PRT agent on each of these boards to compare it with the Dec-POMDP policy.
p91
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p92
(dp93
g10
V      The first two rows in Table~\u005cref{tab:decCompare} show the utility obtained by the MDP-PRT agent and the expected utility of a joint policy generated by a Dec-POMDP.
p94
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p95
(dp96
g10
V   These utilities are averaged over all 6 board configurations and trap probabilities.            \u005cbegin{table}[h]    \u005cvspace{-0.2cm}    \u005ccentering    \u005csmall     \u005cresizebox{6.5cm}{!}{%    \u005cbegin{tabular}{c|c|c|c|c}    {} &5         & 10          & 25          & 50                      \u005c\u005c       \u005chline          \u005chline    Dec-POMDP & \u005cbf{101.8} & \u005cbf{101.46} & \u005cbf{100.2} & \u005cbf{98.13} \u005c\u005c       \u005chline    MDP-PRT (accurate)  & 100.52 & 99.08 & 94.38      & 87.5\u005c\u005c       \u005chline        MDP-PRT (inaccurate)  & 99.75 & 97.46 & 92.13      & 83.67\u005c\u005c           \u005chline    \u005cend{tabular}}    \u005ccaption{The performance of MDP-PRT (with accurate and inaccurate beliefs) and an optimal Dec-POMDP approach.}     \u005clabel{tab:decCompare}       \u005cvspace{-0.3cm}    \u005cend{table}     As expected, the Dec-POMDP always performs better as a result of its additional information at planning time. The difference is statistically significant ($P<0.01$).    As can be seen in the table, increasing communication cost leads to larger differences between the Dec-POMDP and MDP-PRT utilities, a result of the additional communication required by the MDP-PRT agent. When communication cost is up to 10\u005c% of the utility of reaching the goal (100 points), MDP-PRT compares well with the Dec-POMDP policy (less than 3\u005c% difference). When communication cost rises to 25\u005c% or 50\u005c% of goal utility, the difference grows, but the average utility achieved by MDP-PRT is still within 15\u005c% of the optimal utility. Increasing the trap probability decreases the utility obtained by both agents; it also increases the gap between MDP-PRT and the  Dec-POMDP because world states that include more traps and require more communication are more likely.
p97
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p98
(dp99
g10
V \u005csubsection{Inaccurate Beliefs About Others' Plans}The MDP-PRT agent requires knowledge of the probability distributions over the plans of other agents and a way to update this distribution given new information. In practice, it is possible (and likely) that agents will estimate these probabilities inaccurately. To examine the possible effects of such inaccuracies on agents' performance, we ran additional experiments. In these experiments  OBS  knew a probability distribution over PAR's chips, but did not know the actual chips  PAR had. Specifically, it assumed a uniform distribution between 1 to 4 chips of each color.
p100
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p101
(dp102
g10
V Consequently, the initial PRT and updates to the PRT could be inaccurate in their assignment of probabilities to possible plans. The last row of Table~\u005cref{tab:decCompare} shows the performance of the MDP-PRT agent using this inaccurate PRT estimation. As expected, inaccurate beliefs lead to a decrease in utility ($P<0.05$). In these settings, however, the average decrease in utility was small (less than 5\u005c%). Figure~\u005cref{fig:inacc} shows the performance of  three different agents on a particular board configuration that included only one possible trap location with probability 0.5 of a trap  on that location. In this configuration, the Dec-POMDP never communicates and thus always obtains the maximal possible utility. Compared to the MDP-PRT with an accurate PRT representing PAR's possible plans, the inaccurate model communicated more often, resulting in lower utility. Its inaccurate probability estimation assigned higher probability to paths that pass through traps than their actual probability. In general, wrong estimation of the PRT can lead to communicating too much, or communicating too little. 	\u005cbegin{figure}	\u005ccentering	\u005cincludegraphics[width=4.3cm]{figures/fig3cropped.pdf}	%\u005ccaption{The utility achieved by the agents in board configurations with one possible trap location.}	\u005ccaption{Utility for board configurations with one possible trap.}	\u005clabel{fig:inacc}	    \u005cvspace{-0.5cm}	\u005cend{figure}This paper defines the Single Agent in a Team Decision Problem: an agent obtains new, unanticipated, information during a collaborative activity and needs to reason about whether and when to share this information with its teammates. This work is a first step in our effort to develop computer agents to support the care team of children with complex conditions. In this healthcare domain -- a case of dynamic mixed networks comprising people and computational agents -- teams are unlikely to have a fully coordinated joint plan or be able to anticipate all possible events. We thus encode the partial uncertain knowledge about the team's plan in a PRT instead of explicitly modeling all possible world observations (as is typically required in DT planning approaches). The PRT compactly represents many different possible plans and thus (indirectly) many possible observations as group activities evolve. For example, if an agent learns that its teammate chose a specific recipe, it can eliminate other possibilities at that OR node. Generally, either low-level or complex actions might be ``observed'' and the agent's beliefs can be revised by modifying the PRT accordingly. PRTs do not however support reasoning about {\u005cem when} to share new observations which  DT approaches do. We thus propose an integrated BDI/DT approach. In this approach, an MDP reasons about communication using the PRT representation of other agents' possible plans (in the context provided by the SharedPlan).
p103
sg17
I00
sg18
Nsg19
Nsba(iversion
Paragraph
p104
(dp105
g10
VTwo extensions of this work are required to use the MDP-PRT approach for agents operating in  mixed-networks, each a general multi-agent system challenge. To solve SATD for larger groups of agents requires deciding with whom to communicate, which itself requires taking into account interactions among agents' plans. We intend to use the decomposability of a team's plan to enable focusing on relevant agents and relevant aspects of their plans. The second challenge is developing methods for agents to acquire knowledge about other agents' possible plans. We are exploring several possibilities in the healthcare domain. Agents can infer an initial belief about their teammates' plans based on the partial high-level plan defined by the team. For example,  there are often guidelines for treating different conditions which can be used to reason about alternative plans of care providers. Also, agents may be able to observe some of their teammates' actions and refine their beliefs using plan recognition techniques. For teams collaborating over a long time horizon, learning could be used to estimate others' plans and how they change with new information.
p106
sg17
I00
sg18
Nsg19
NsbasS'author'
p107
S'Ofra'
p108
sbasS'title'
p109
S'Ofra_SATD'
p110
sb.